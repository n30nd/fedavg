{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK_DATASET = \"/media/namvq/Data/chest_xray\"\n",
    "# LINK_DATASET = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\n",
    "NUM_WORKERS = 6\n",
    "# BASE_FOLDER_NOISE = \"/kaggle/input/chest-xray-noise-60-partitions\"\n",
    "BASE_FOLDER_NOISE = \"/media/namvq/Data/code_chinh_sua/fedavg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKEND:  Agg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Partition the data and create the dataloaders.\"\"\"\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Chuyển sang backend không cần GUI\n",
    "\n",
    "print('BACKEND: ', matplotlib.get_backend())\n",
    "# def get_custom_dataset(data_path: str = \"/media/namvq/Data/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     transform = Compose([\n",
    "#         Resize((100, 100)),\n",
    "#         Grayscale(num_output_channels=1),\n",
    "#         ToTensor()\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=transform)\n",
    "#     return trainset, testset\n",
    "\n",
    "# def get_custom_dataset(data_path: str = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=transform)\n",
    "#     return trainset, testset\n",
    "\n",
    "# def get_custom_dataset(data_path: str = \"/media/namvq/Data/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     train_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     test_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "#     return trainset, testset\n",
    "def get_custom_dataset(data_path: str = LINK_DATASET):\n",
    "    \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),  # Kích thước ảnh cho VGG\n",
    "        transforms.RandomAffine(degrees=0, shear=10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.2, 0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((150, 150)),  # Kích thước ảnh cho VGG\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "    testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "    return trainset, testset\n",
    "\n",
    "#Lay tap val goc co 16 anh thoi\n",
    "def get_val_dataloader(batch_size: int = 10):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    valset = ImageFolder(os.path.join(LINK_DATASET, 'val'), transform=val_transform)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    return valloader\n",
    "\n",
    "def prepare_dataset_for_centralized_train(batch_size: int, val_ratio: float = 0.1, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "\n",
    "def prepare_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, alpha: float = 100, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate non-IID partitions using Dirichlet distribution.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    \n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "    \n",
    "    # Generate Dirichlet distribution for each class\n",
    "    class_indices = [np.where(train_labels == i)[0] for i in range(len(np.unique(train_labels)))]\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    \n",
    "    for class_idx in class_indices:\n",
    "        np.random.shuffle(class_idx)\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(class_idx)).astype(int)[:-1]\n",
    "        class_partitions = np.split(class_idx, proportions)\n",
    "        for i in range(num_partitions):\n",
    "            partition_indices[i].extend(class_partitions[i])\n",
    "    \n",
    "    # Create Subsets for each partition\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "    \n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "\n",
    "    valsets = random_split(valset, partition_len_val, torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_partitioned_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, num_labels_each_party: int = 1, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    times = [0 for i in range(num_labels)]\n",
    "    contain = []\n",
    "    #Phan label cho cac client\n",
    "    for i in range(num_partitions):\n",
    "        current = [i%num_labels]\n",
    "        times[i%num_labels] += 1\n",
    "        if num_labels_each_party > 1:\n",
    "            current.append(1-i%num_labels)\n",
    "            times[1-i%num_labels] += 1\n",
    "        contain.append(current)\n",
    "    print(times)\n",
    "    print(contain)\n",
    "    # Create Subsets for each partition\n",
    "\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    for i in range(num_labels):\n",
    "        idx_i = np.where(train_labels == i)[0]  # Get indices of label i in train_labels\n",
    "        idx_i = [trainset.indices[j] for j in idx_i]  # Convert indices to indices in trainset\n",
    "        # #print label of idx_i\n",
    "        # print(\"Label of idx: \", i)\n",
    "        # for j in range(len(idx_i)):\n",
    "        #     idx_in_dataset = trainset.indices[idx_i[j]]\n",
    "        #     print(trainset.dataset.targets[idx_in_dataset])\n",
    "        np.random.shuffle(idx_i)\n",
    "        split = np.array_split(idx_i, times[i])\n",
    "        ids = 0\n",
    "        for j in range(num_partitions):\n",
    "            if i in contain[j]:\n",
    "                partition_indices[j].extend(split[ids])\n",
    "                ids += 1\n",
    "    \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    # #print label of client 0\n",
    "    # print(\"Client 0\")\n",
    "    # for i in range(len(trainsets[0])):\n",
    "    #     print(trainsets[0][i][1])\n",
    "\n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_imbalance_label_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 0.5, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    min_size = 0\n",
    "    min_require_size = 2\n",
    "\n",
    "    N = len(trainset)\n",
    "\n",
    "\n",
    "    while(min_size < min_require_size):\n",
    "        partition_indices = [[] for _ in range(num_partitions)]\n",
    "        for label in range(num_labels):\n",
    "            idx_label = np.where(train_labels == label)[0]\n",
    "            idx_label = [trainset.indices[j] for j in idx_label]\n",
    "            np.random.shuffle(idx_label)\n",
    "\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "            # proportions = np.array( [p * len(idx_j) < N/num_partitions] for p, idx_j in zip(proportions, partition_indices))\n",
    "            proportions = np.array([p if p * len(idx_j) < N / num_partitions else 0 for p, idx_j in zip(proportions, partition_indices)])\n",
    "\n",
    "            proportions = proportions / np.sum(proportions)\n",
    "            proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "\n",
    "            partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, np.split(idx_label, proportions))]\n",
    "            min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "\n",
    "def apply_gaussian_noise(tensor, std_dev):\n",
    "    noise = torch.randn_like(tensor) * std_dev\n",
    "    return tensor + noise\n",
    "\n",
    "# Hàm đảo ngược chuẩn hóa\n",
    "def unnormalize_image(image_tensor, mean, std):\n",
    "    # Đảo ngược Normalize: (image * std) + mean\n",
    "    for t, m, s in zip(image_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Thực hiện từng kênh\n",
    "    return image_tensor\n",
    "\n",
    "# Hàm hiển thị ảnh từ một tensor\n",
    "def display_image(image_tensor, mean, std):\n",
    "    # Đảo ngược chuẩn hóa\n",
    "    image_tensor = unnormalize_image(image_tensor, mean, std)\n",
    "    # Chuyển tensor thành NumPy array và điều chỉnh thứ tự kênh màu (CHW -> HWC)\n",
    "    image_numpy = image_tensor.permute(1, 2, 0).numpy()\n",
    "    # Cắt giá trị ảnh về phạm vi [0, 1] để hiển thị đúng\n",
    "    image_numpy = image_numpy.clip(0, 1)\n",
    "    # Trả về ảnh NumPy\n",
    "    return image_numpy\n",
    "\n",
    "# def prepare_noise_based_imbalance(num_partitions: int, batch_size: int, val_ratio: float = 0.1, sigma: float = 0.05, seed: int = 42):\n",
    "#     \"\"\"\n",
    "#     Chia du lieu ngau nhien va deu cho cac ben, sau do them noise vao cac ben\n",
    "#     moi ben i co noise khac nhau Gauss(0, sigma*i/N)\n",
    "#     \"\"\"\n",
    "#     trainset, testset = get_custom_dataset()\n",
    "#     num_train = int((1 - val_ratio) * len(trainset))\n",
    "#     num_val = len(trainset) - num_train\n",
    "#     trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "#     indices = trainset.indices\n",
    "\n",
    "#     np.random.shuffle(indices)\n",
    "\n",
    "#     partition_indices = np.array_split(indices, num_partitions)\n",
    "\n",
    "#     train_partitions = []\n",
    "\n",
    "#     for i, part_indices in enumerate(partition_indices):\n",
    "#         partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "#         partition_set = Subset(trainset.dataset, part_indices)\n",
    "        \n",
    "#         noisy_samples = [apply_gaussian_noise(sample[0], partition_std_dev) for sample in partition_set]\n",
    "#         noisy_dataset = [(noisy_samples[j], trainset.dataset[part_indices[j]][1]) for j in range(len(part_indices))]\n",
    "#         # train_partitions.append((noisy_samples, [sample[1] for sample in partition_set]))\n",
    "#         train_partitions.append(noisy_dataset)\n",
    "#     trainloaders = [DataLoader(train_partitions[i], batch_size=batch_size, shuffle=True, num_workers=4) for i in range(num_partitions)]\n",
    "#     partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "#     for i in range(len(valset) % num_partitions):\n",
    "#         partition_len_val[i] += 1\n",
    "    \n",
    "#     valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "#     valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "#     testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ####\n",
    "#     class_distributions = []\n",
    "#     for i, trainloader in enumerate(trainloaders):\n",
    "#         class_counts = Counter()\n",
    "#         for _, labels in trainloader:\n",
    "#             class_counts.update(labels.numpy())\n",
    "#         class_distributions.append(class_counts)\n",
    "#         print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "#     partitions = range(num_partitions)\n",
    "#     class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "#     class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "#     bar_width = 0.5\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "#     plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "#     plt.xlabel('Partition')\n",
    "#     plt.ylabel('Number of Samples')\n",
    "#     plt.title('Class Distribution in Each Partition')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     # plt.show()\n",
    "#     #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "#     output_dir = 'running_outputs'\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "#     plt.close()\n",
    "\n",
    "#     #Lưu ảnh nhiễu vào running_outputs\n",
    "#     # Mean và std từ Normalize\n",
    "#     mean = [0.485, 0.456, 0.406]\n",
    "#     std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#     # Tạo thư mục lưu ảnh nếu chưa tồn tại\n",
    "#     output_dir = \"running_outputs\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Khởi tạo một lưới 10x6 để hiển thị ảnh\n",
    "#     fig, axes = plt.subplots(10, 6, figsize=(15, 25))\n",
    "\n",
    "#     # Duyệt qua 60 trainloaders và hiển thị ảnh đầu tiên\n",
    "#     for i, trainloader in enumerate(trainloaders[:num_partitions]):\n",
    "#         # Lấy ảnh đầu tiên từ trainloader\n",
    "#         image_tensor = trainloader.dataset[0][0].clone()  # Clone để tránh thay đổi dữ liệu gốc\n",
    "        \n",
    "#         # Tìm vị trí hàng, cột trong lưới\n",
    "#         row, col = divmod(i, 6)\n",
    "#         plt.sca(axes[row, col])  # Đặt trục hiện tại là vị trí hàng, cột trong lưới\n",
    "        \n",
    "#         # Hiển thị ảnh\n",
    "#         image_numpy = display_image(image_tensor, mean, std)\n",
    "#         axes[row, col].imshow(image_numpy)\n",
    "#         axes[row, col].axis('off')\n",
    "#     plt.title(f\"Noise image with sigma from {sigma * 1 / num_partitions} to {sigma}\")\n",
    "#     # Điều chỉnh layout để không bị chồng lấn\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     # Lưu ảnh thay vì hiển thị\n",
    "#     output_path = os.path.join(output_dir, \"image_noise.png\")\n",
    "#     plt.savefig(output_path, dpi=300)  # Lưu ảnh với chất lượng cao\n",
    "\n",
    "#     plt.close()  # Đóng figure\n",
    "\n",
    "#     print(f\"Ảnh đã được lưu tại {output_path}\")\n",
    "\n",
    "#     print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "# ###\n",
    "#     return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_noise_based_imbalance(num_partitions: int, batch_size: int, val_ratio: float = 0.1, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Chia dữ liệu ngẫu nhiên và đều cho các bên, sau đó thêm noise vào các bên.\n",
    "    Mỗi bên i có noise khác nhau Gauss(0, sigma*i/N). Nếu dữ liệu đã tồn tại, tải từ thư mục dataset_noise_{sigma}.\n",
    "    \"\"\"\n",
    "    # noise_dir = f'chest_xray_noise_{sigma}'\n",
    "    # noise_dir = f'/kaggle/input/chest-xray-noise-60-partitions/chest_xray_noise_{sigma}'\n",
    "    noise_dir = f\"{BASE_FOLDER_NOISE}/chest_xray_noise_{sigma}\"\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    if os.path.exists(noise_dir):\n",
    "        print(f\"Loading noisy dataset from {noise_dir}...\")\n",
    "        # Sử dụng ImageFolder để tải dữ liệu đã được thêm nhiễu với transform phù hợp\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) for i in range(num_partitions)]\n",
    "        \n",
    "        # Tải val và test set như bình thường\n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "        print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "    else:\n",
    "        print(f\"Creating noisy dataset and saving to {noise_dir}...\")\n",
    "        os.makedirs(noise_dir, exist_ok=True)\n",
    "        \n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "        indices = trainset.indices\n",
    "        np.random.shuffle(indices)\n",
    "        partition_indices = np.array_split(indices, num_partitions)\n",
    "    \n",
    "        # Mean và std từ Normalize đã được định nghĩa trước\n",
    "        for i, part_indices in enumerate(partition_indices):\n",
    "            partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "            partition_set = Subset(trainset.dataset, part_indices)\n",
    "            \n",
    "            # Tạo thư mục cho partition và các lớp\n",
    "            partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "            os.makedirs(partition_dir, exist_ok=True)\n",
    "            class_dirs = {}\n",
    "            for _, label in partition_set:\n",
    "                if label not in class_dirs:\n",
    "                    class_dirs[label] = os.path.join(partition_dir, f'class_{label}')\n",
    "                    os.makedirs(class_dirs[label], exist_ok=True)\n",
    "            \n",
    "            for j, (image, label) in enumerate(partition_set):\n",
    "                noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "                # Đảo ngược chuẩn hóa để lưu ảnh đúng định dạng\n",
    "                noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "                # Chuyển tensor thành PIL Image\n",
    "                noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "                # Lưu ảnh với tên duy nhất\n",
    "                image_filename = f'image_{j}.png'\n",
    "                noisy_image_pil.save(os.path.join(class_dirs[label], image_filename))\n",
    "        \n",
    "        # Tải dữ liệu từ thư mục đã lưu với transform phù hợp\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) for i in range(num_partitions)]\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "    \n",
    "    # Phân tích phân bố lớp\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i].get(0, 0) for i in partitions]\n",
    "    class_1_counts = [class_distributions[i].get(1, 0) for i in partitions]\n",
    "    \n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Lưu ảnh nhiễu vào running_outputs\n",
    "    # Tạo thư mục lưu ảnh nếu chưa tồn tại\n",
    "    output_dir = \"running_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Khởi tạo một lưới 10x6 để hiển thị ảnh\n",
    "    fig, axes = plt.subplots(10, 6, figsize=(15, 25))\n",
    "    \n",
    "    # Duyệt qua trainloaders và hiển thị ảnh đầu tiên từ mỗi partition\n",
    "    for i, trainloader in enumerate(trainloaders[:min(num_partitions, 60)]):\n",
    "        if len(trainloader.dataset) == 0:\n",
    "            continue\n",
    "        # Lấy ảnh đầu tiên từ trainloader\n",
    "        image_tensor, label = trainloader.dataset[0]\n",
    "        \n",
    "        # Tìm vị trí hàng, cột trong lưới\n",
    "        row, col = divmod(i, 6)\n",
    "        if row >= 10:\n",
    "            break  # Chỉ hiển thị tối đa 60 ảnh\n",
    "        # Hiển thị ảnh\n",
    "        image_numpy = unnormalize_image(image_tensor.clone(), mean, std).permute(1, 2, 0).numpy().clip(0, 1)\n",
    "        axes[row, col].imshow(image_numpy)\n",
    "        axes[row, col].axis('off')\n",
    "    # Điều chỉnh layout để không bị chồng lấn\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Lưu ảnh thay vì hiển thị\n",
    "    output_path = os.path.join(output_dir, \"image_noise.png\")\n",
    "    plt.savefig(output_path, dpi=300)  # Lưu ảnh với chất lượng cao\n",
    "    \n",
    "    plt.close()  # Đóng figure\n",
    "    \n",
    "    print(f\"Ảnh minh họa đã được lưu tại {output_path}\")\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    \n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "def prepare_quantity_skew_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 10, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    all_indices = trainset.indices\n",
    "\n",
    "    min_size = 0\n",
    "    while min_size < 1:\n",
    "        proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(all_indices)).astype(int)[:-1]\n",
    "\n",
    "        partition_indices = np.split(all_indices, proportions)\n",
    "\n",
    "        min_size = min([len(partition) for partition in partition_indices])\n",
    "        print('Partition sizes:', [len(partition) for partition in partition_indices])\n",
    "        print('Min partition size:', min_size)\n",
    "\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    config: DictConfig,\n",
    "    num_clients: int,\n",
    "    val_ratio: float = 0.1,\n",
    "    seed: Optional[int] = 42,\n",
    ") -> Tuple[List[DataLoader], List[DataLoader], DataLoader]:\n",
    "    \"\"\"Create the dataloaders to be fed into the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: DictConfig\n",
    "        Parameterises the dataset partitioning process\n",
    "    num_clients : int\n",
    "        The number of clients that hold a part of the data\n",
    "    val_ratio : float, optional\n",
    "        The ratio of training data that will be used for validation (between 0 and 1),\n",
    "        by default 0.1\n",
    "    seed : int, optional\n",
    "        Used to set a fix seed to replicate experiments, by default 42\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[DataLoader, DataLoader, DataLoader]\n",
    "        The DataLoaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    print(f\"Dataset partitioning config: {config}\")\n",
    "    batch_size = -1\n",
    "    print('config:' , config)\n",
    "    if \"batch_size\" in config:\n",
    "        batch_size = config.batch_size\n",
    "    elif \"batch_size_ratio\" in config:\n",
    "        batch_size_ratio = config.batch_size_ratio\n",
    "    else:\n",
    "        raise ValueError\n",
    "    partitioning = \"\"\n",
    "    \n",
    "    if \"partitioning\" in config:\n",
    "        partitioning = config.partitioning\n",
    "\n",
    "    # partition the data\n",
    "    if partitioning == \"imbalance_label\":\n",
    "        return prepare_partitioned_dataset(num_clients, batch_size, val_ratio, config.labels_per_client, config.seed)\n",
    "\n",
    "    if partitioning == \"imbalance_label_dirichlet\":\n",
    "        return prepare_imbalance_label_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "\n",
    "    if partitioning == \"noise_based_imbalance\":\n",
    "        return prepare_noise_based_imbalance(num_clients, batch_size, val_ratio, config.sigma, config.seed)\n",
    "\n",
    "    if partitioning == \"quantity_skew_dirichlet\":\n",
    "        return prepare_quantity_skew_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading noisy dataset from /media/namvq/Data/code_chinh_sua/fedavg/chest_xray_noise_0.1...\n",
      "Dữ liệu đã được tải thành công từ thư mục lưu trữ.\n",
      "Partition 0 class distribution: {1: 34, 0: 14}\n",
      "Partition 1 class distribution: {1: 39, 0: 9}\n",
      "Partition 2 class distribution: {1: 33, 0: 15}\n",
      "Partition 3 class distribution: {0: 13, 1: 34}\n",
      "Ảnh minh họa đã được lưu tại running_outputs/image_noise.png\n",
      "Number of train samples: 191, val samples: 0, test samples: 129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<torch.utils.data.dataloader.DataLoader at 0x71bf1c144310>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf1c144430>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf1c144550>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf1c1445e0>],\n",
       " [<torch.utils.data.dataloader.DataLoader at 0x71bf1c144790>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf1c1447f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf1c1448e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf1c1449d0>],\n",
       " <torch.utils.data.dataloader.DataLoader at 0x71bf1c1443a0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_noise_based_imbalance(4, 10, 0, 0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imbalanced_and_noisy_data(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 0.5, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"Combine label-imbalanced partitioning and Gaussian noise application.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels (Dirichlet distribution)\n",
    "    num_labels = len(np.unique(train_labels))\n",
    "    min_size = 0\n",
    "    min_require_size = 2\n",
    "    N = len(trainset)\n",
    "\n",
    "    while min_size < min_require_size:\n",
    "        partition_indices = [[] for _ in range(num_partitions)]\n",
    "        for label in range(num_labels):\n",
    "            idx_label = np.where(train_labels == label)[0]\n",
    "            idx_label = [trainset.indices[j] for j in idx_label]\n",
    "            np.random.shuffle(idx_label)\n",
    "\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "            proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "\n",
    "            partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, np.split(idx_label, proportions))]\n",
    "        min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    # Add Gaussian noise to each partition\n",
    "    noisy_partitions = []\n",
    "    for i, train_partition in enumerate(trainsets):\n",
    "        partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "        noisy_data = []\n",
    "        for image, label in train_partition:\n",
    "            noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "            noisy_data.append((noisy_image, label))\n",
    "        noisy_partitions.append(noisy_data)\n",
    "\n",
    "    # Partition validation set equally\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Create DataLoaders\n",
    "    trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) for part in noisy_partitions]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "    # Analyze class distributions\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "\n",
    "    # Plot class distributions\n",
    "    partitions = range(num_partitions)\n",
    "    class_counts = [{cls: class_distributions[i].get(cls, 0) for cls in range(num_labels)} for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for cls in range(num_labels):\n",
    "        counts = [class_counts[i].get(cls, 0) for i in partitions]\n",
    "        plt.bar(partitions, counts, bar_width, label=f'Class {cls}')\n",
    "\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition_combined.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0 class distribution: {0: 29, 1: 7}\n",
      "Partition 1 class distribution: {0: 8, 1: 5}\n",
      "Partition 2 class distribution: {1: 124, 0: 1}\n",
      "Partition 3 class distribution: {0: 13, 1: 4}\n",
      "Number of train samples: 191, val samples: 0, test samples: 129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<torch.utils.data.dataloader.DataLoader at 0x71bf44f056f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf44f054b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4544f130>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4544f730>],\n",
       " [<torch.utils.data.dataloader.DataLoader at 0x71bf4544f220>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4544f760>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4544f7c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4544f8e0>],\n",
       " <torch.utils.data.dataloader.DataLoader at 0x71bf44dcad70>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_imbalanced_and_noisy_data(4, 10, 0, 0.5, 0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating noisy dataset and saving to /media/namvq/Data/code_chinh_sua/fedavg/chest_xray_noise_label_drl0.1...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ImageFolder' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprepare_combined_imbalance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 91\u001b[0m, in \u001b[0;36mprepare_combined_imbalance\u001b[0;34m(num_partitions, batch_size, val_ratio, beta, sigma, seed)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Áp dụng nhiễu Gauss cho từng phân vùng và lưu vào thư mục\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, indices \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(partition_indices):\n\u001b[0;32m---> 91\u001b[0m     partition_set \u001b[38;5;241m=\u001b[39m Subset(\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m, indices)\n\u001b[1;32m     92\u001b[0m     partition_std_dev \u001b[38;5;241m=\u001b[39m sigma \u001b[38;5;241m*\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m num_partitions\n\u001b[1;32m     94\u001b[0m     partition_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(noise_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageFolder' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "prepare_combined_imbalance(4, 10, 0, 0.5, 0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_imbalanced_and_noisy_data(num_partitions: int, batch_size: int, \n",
    "#                                       val_ratio: float = 0.1, beta: float = 0.5, \n",
    "#                                       sigma: float = 0.05, seed: int = 42):\n",
    "#     \"\"\"\n",
    "#     Phân chia dữ liệu với phân phối không cân bằng và thêm nhiễu Gaussian.\n",
    "#     Nếu dữ liệu đã được lưu, tải từ thư mục lưu trữ; nếu chưa, thực hiện phân chia và lưu.\n",
    "#     \"\"\"\n",
    "#     # Định nghĩa thư mục lưu trữ dựa trên các tham số\n",
    "#     noise_dir = f\"data_partition_combined_{num_partitions}_beta_{beta}_sigma_{sigma}\"\n",
    "#     mean = [0.485, 0.456, 0.406]\n",
    "#     std = [0.229, 0.224, 0.225]\n",
    "#     noisy_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std)\n",
    "#     ])\n",
    "    \n",
    "#     if os.path.exists(noise_dir):\n",
    "#         print(f\"Loading partitioned and noisy dataset from {noise_dir}...\")\n",
    "#         # Tải các partition đã lưu bằng ImageFolder\n",
    "#         train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "#                             for i in range(num_partitions)]\n",
    "        \n",
    "#         # Tải val và test set như bình thường\n",
    "#         trainset, testset = get_custom_dataset()\n",
    "#         num_train = int((1 - val_ratio) * len(trainset))\n",
    "#         num_val = len(trainset) - num_train\n",
    "#         _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "#         # Chia valset thành các partition\n",
    "#         partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "#         for i in range(len(valset) % num_partitions):\n",
    "#             partition_len_val[i] += 1\n",
    "#         valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "#         # Tạo DataLoaders\n",
    "#         trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "#                        for part in train_partitions]\n",
    "#         valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "#                       for vs in valsets]\n",
    "#         testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "#         print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "#     else:\n",
    "#         print(f\"Creating partitioned and noisy dataset and saving to {noise_dir}...\")\n",
    "#         os.makedirs(noise_dir, exist_ok=True)\n",
    "        \n",
    "#         trainset, testset = get_custom_dataset()\n",
    "#         num_train = int((1 - val_ratio) * len(trainset))\n",
    "#         num_val = len(trainset) - num_train\n",
    "#         train_subset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "#         # Lấy nhãn của toàn bộ trainset\n",
    "#         train_labels = np.array([train_subset.dataset.targets[i] for i in train_subset.indices])\n",
    "#         num_labels = len(np.unique(train_labels))\n",
    "#         min_size = 0\n",
    "#         min_require_size = 2\n",
    "#         N = len(train_subset)\n",
    "        \n",
    "#         # Phân chia dữ liệu theo phân phối Dirichlet\n",
    "#         while min_size < min_require_size:\n",
    "#             partition_indices = [[] for _ in range(num_partitions)]\n",
    "#             for label in range(num_labels):\n",
    "#                 idx_label = np.where(train_labels == label)[0]\n",
    "#                 idx_label = [train_subset.indices[j] for j in idx_label]\n",
    "#                 np.random.shuffle(idx_label)\n",
    "                \n",
    "#                 proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "#                 proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "                \n",
    "#                 splits = np.split(idx_label, proportions)\n",
    "#                 partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, splits)]\n",
    "#             min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "#         # Lưu các partition với nhiễu Gaussian\n",
    "#         for i, indices in enumerate(partition_indices):\n",
    "#             partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "#             os.makedirs(partition_dir, exist_ok=True)\n",
    "#             class_dirs = {}\n",
    "#             for idx in indices:\n",
    "#                 _, label = train_subset.dataset[idx]\n",
    "#                 if label not in class_dirs:\n",
    "#                     class_dirs[label] = os.path.join(partition_dir, f'class_{label}')\n",
    "#                     os.makedirs(class_dirs[label], exist_ok=True)\n",
    "            \n",
    "#             # Thêm nhiễu và lưu ảnh\n",
    "#             partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "#             for j, idx in enumerate(indices):\n",
    "#                 image, label = train_subset.dataset[idx]\n",
    "#                 noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "#                 noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "#                 noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "#                 image_filename = f'image_{j}.png'\n",
    "#                 noisy_image_pil.save(os.path.join(class_dirs[label], image_filename))\n",
    "        \n",
    "#         # Tải các partition đã lưu bằng ImageFolder\n",
    "#         train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "#                             for i in range(num_partitions)]\n",
    "        \n",
    "#         # Chia valset thành các partition\n",
    "#         partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "#         for i in range(len(valset) % num_partitions):\n",
    "#             partition_len_val[i] += 1\n",
    "#         valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "#         # Tạo DataLoaders\n",
    "#         trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "#                        for part in train_partitions]\n",
    "#         valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "#                       for vs in valsets]\n",
    "#         testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "#         print(f\"Dữ liệu đã được phân chia và lưu tại {noise_dir}\")\n",
    "    \n",
    "#     # Phân tích phân bố lớp\n",
    "#     class_distributions = []\n",
    "#     for i, trainloader in enumerate(trainloaders):\n",
    "#         class_counts = Counter()\n",
    "#         for _, labels in trainloader:\n",
    "#             class_counts.update(labels.numpy())\n",
    "#         class_distributions.append(class_counts)\n",
    "#         print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "#     # Vẽ biểu đồ phân bố lớp\n",
    "#     partitions = range(num_partitions)\n",
    "#     class_counts = [{cls: class_distributions[i].get(cls, 0) for cls in range(num_labels)} for i in partitions]\n",
    "    \n",
    "#     bar_width = 0.5\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     for cls in range(num_labels):\n",
    "#         counts = [class_counts[i].get(cls, 0) for i in partitions]\n",
    "#         plt.bar(partitions, counts, bar_width, label=f'Class {cls}')\n",
    "    \n",
    "#     plt.xlabel('Partition')\n",
    "#     plt.ylabel('Number of Samples')\n",
    "#     plt.title('Class Distribution in Each Partition')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     output_dir = 'running_outputs'\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     plt.savefig(os.path.join(output_dir, 'data_partition_combined.png'))\n",
    "#     plt.close()\n",
    "    \n",
    "#     print(f'Number of train samples: {len(train_subset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    \n",
    "#     return trainloaders, valloaders, testloader\n",
    "def prepare_imbalanced_and_noisy_data(num_partitions: int, batch_size: int, \n",
    "                                      val_ratio: float = 0.1, beta: float = 0.5, \n",
    "                                      sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Phân chia dữ liệu với phân phối không cân bằng và thêm nhiễu Gaussian.\n",
    "    Nếu dữ liệu đã được lưu, tải từ thư mục lưu trữ; nếu chưa, thực hiện phân chia và lưu.\n",
    "    \"\"\"\n",
    "    # Định nghĩa thư mục lưu trữ dựa trên các tham số\n",
    "    # noise_dir = f\"data_partition_combined_{num_partitions}_beta_{beta}_sigma_{sigma}\"\n",
    "    noise_dir = f\"{BASE_FOLDER_NOISE}/chest_xray_noise_drl_label{beta}_{sigma}\"\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    if os.path.exists(noise_dir):\n",
    "        print(f\"Loading partitioned and noisy dataset from {noise_dir}...\")\n",
    "        # Tải các partition đã lưu bằng ImageFolder\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "                            for i in range(num_partitions)]\n",
    "        \n",
    "        # Lấy số lượng lớp từ một trong các partition\n",
    "        if len(train_partitions) > 0:\n",
    "            num_labels = len(train_partitions[0].classes)\n",
    "        else:\n",
    "            raise ValueError(\"Không tìm thấy partition nào trong thư mục lưu trữ.\")\n",
    "        \n",
    "        # Tải val và test set như bình thường\n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "                       for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "                      for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "        print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "    else:\n",
    "        print(f\"Creating partitioned and noisy dataset and saving to {noise_dir}...\")\n",
    "        os.makedirs(noise_dir, exist_ok=True)\n",
    "        \n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        train_subset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Lấy nhãn của toàn bộ trainset\n",
    "        train_labels = np.array([train_subset.dataset.targets[i] for i in train_subset.indices])\n",
    "        num_labels = len(np.unique(train_labels))\n",
    "        min_size = 0\n",
    "        min_require_size = 2\n",
    "        N = len(train_subset)\n",
    "        \n",
    "        # Phân chia dữ liệu theo phân phối Dirichlet\n",
    "        while min_size < min_require_size:\n",
    "            partition_indices = [[] for _ in range(num_partitions)]\n",
    "            for label in range(num_labels):\n",
    "                idx_label = np.where(train_labels == label)[0]\n",
    "                idx_label = [train_subset.indices[j] for j in idx_label]\n",
    "                np.random.shuffle(idx_label)\n",
    "                \n",
    "                proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "                proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "                \n",
    "                splits = np.split(idx_label, proportions)\n",
    "                partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, splits)]\n",
    "            min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "        # Lưu các partition với nhiễu Gaussian\n",
    "        for i, indices in enumerate(partition_indices):\n",
    "            partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "            os.makedirs(partition_dir, exist_ok=True)\n",
    "            class_dirs = {}\n",
    "            for idx in indices:\n",
    "                _, label = train_subset.dataset[idx]\n",
    "                if label not in class_dirs:\n",
    "                    class_dirs[label] = os.path.join(partition_dir, f'class_{label}')\n",
    "                    os.makedirs(class_dirs[label], exist_ok=True)\n",
    "            \n",
    "            # Thêm nhiễu và lưu ảnh\n",
    "            partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "            for j, idx in enumerate(indices):\n",
    "                image, label = train_subset.dataset[idx]\n",
    "                noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "                noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "                noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "                image_filename = f'image_{j}.png'\n",
    "                noisy_image_pil.save(os.path.join(class_dirs[label], image_filename))\n",
    "        \n",
    "        # Tải các partition đã lưu bằng ImageFolder\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "                            for i in range(num_partitions)]\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "                       for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "                      for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "        print(f\"Dữ liệu đã được phân chia và lưu tại {noise_dir}\")\n",
    "    \n",
    "    # Phân tích phân bố lớp\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    # Vẽ biểu đồ phân bố lớp\n",
    "    partitions = range(num_partitions)\n",
    "    class_counts_list = []\n",
    "    for i in partitions:\n",
    "        counts = {cls: class_distributions[i].get(cls, 0) for cls in range(num_labels)}\n",
    "        class_counts_list.append(counts)\n",
    "    \n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bottom = np.zeros(num_partitions)\n",
    "    colors = plt.cm.tab10.colors  # Sử dụng bảng màu có sẵn\n",
    "    \n",
    "    for cls in range(num_labels):\n",
    "        counts = [class_counts_list[i].get(cls, 0) for i in partitions]\n",
    "        plt.bar(partitions, counts, bar_width, bottom=bottom, label=f'Class {cls}', color=colors[cls % len(colors)])\n",
    "        bottom += counts\n",
    "    \n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition_combined.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f'Number of train samples: {sum(len(loader.dataset) for loader in trainloaders)}, '\n",
    "          f'val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    \n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating partitioned and noisy dataset and saving to data_partition_combined_4_beta_0.5_sigma_1...\n",
      "Dữ liệu đã được phân chia và lưu tại data_partition_combined_4_beta_0.5_sigma_1\n",
      "Partition 0 class distribution: {1: 83, 0: 7}\n",
      "Partition 1 class distribution: {0: 22, 1: 49}\n",
      "Partition 2 class distribution: {0: 15, 1: 3}\n",
      "Partition 3 class distribution: {0: 7, 1: 5}\n",
      "Number of train samples: 191, val samples: 0, test samples: 129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<torch.utils.data.dataloader.DataLoader at 0x71bf4441d210>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4441c6d0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4441e770>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4441d030>],\n",
       " [<torch.utils.data.dataloader.DataLoader at 0x71bf4441ea40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4441e470>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4441e4a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x71bf4441ec20>],\n",
       " <torch.utils.data.dataloader.DataLoader at 0x71bf454c81f0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_imbalanced_and_noisy_data(4, 10, 0, 0.5, 1, 42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niid-bench-orV17zke-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
