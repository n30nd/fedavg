{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK_DATASET = \"/media/namvq/Data/chest_xray\"\n",
    "# LINK_DATASET = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\n",
    "NUM_WORKERS = 6\n",
    "# BASE_FOLDER_NOISE = \"/kaggle/input/chest-xray-noise-60-partitions\"\n",
    "BASE_FOLDER_NOISE = \"/media/namvq/Data/code_chinh_sua/fedavg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKEND:  Agg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Partition the data and create the dataloaders.\"\"\"\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Chuyển sang backend không cần GUI\n",
    "\n",
    "print('BACKEND: ', matplotlib.get_backend())\n",
    "# def get_custom_dataset(data_path: str = \"/media/namvq/Data/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     transform = Compose([\n",
    "#         Resize((100, 100)),\n",
    "#         Grayscale(num_output_channels=1),\n",
    "#         ToTensor()\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=transform)\n",
    "#     return trainset, testset\n",
    "\n",
    "# def get_custom_dataset(data_path: str = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=transform)\n",
    "#     return trainset, testset\n",
    "\n",
    "# def get_custom_dataset(data_path: str = \"/media/namvq/Data/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     train_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     test_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "#     return trainset, testset\n",
    "def get_custom_dataset(data_path: str = LINK_DATASET):\n",
    "    \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),  # Kích thước ảnh cho VGG\n",
    "        transforms.RandomAffine(degrees=0, shear=10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.2, 0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((150, 150)),  # Kích thước ảnh cho VGG\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "    testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "    return trainset, testset\n",
    "\n",
    "#Lay tap val goc co 16 anh thoi\n",
    "def get_val_dataloader(batch_size: int = 10):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    valset = ImageFolder(os.path.join(LINK_DATASET, 'val'), transform=val_transform)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    return valloader\n",
    "\n",
    "def prepare_dataset_for_centralized_train(batch_size: int, val_ratio: float = 0.1, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "\n",
    "def prepare_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, alpha: float = 100, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate non-IID partitions using Dirichlet distribution.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    \n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "    \n",
    "    # Generate Dirichlet distribution for each class\n",
    "    class_indices = [np.where(train_labels == i)[0] for i in range(len(np.unique(train_labels)))]\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    \n",
    "    for class_idx in class_indices:\n",
    "        np.random.shuffle(class_idx)\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(class_idx)).astype(int)[:-1]\n",
    "        class_partitions = np.split(class_idx, proportions)\n",
    "        for i in range(num_partitions):\n",
    "            partition_indices[i].extend(class_partitions[i])\n",
    "    \n",
    "    # Create Subsets for each partition\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "    \n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "\n",
    "    valsets = random_split(valset, partition_len_val, torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_partitioned_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, num_labels_each_party: int = 1, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    times = [0 for i in range(num_labels)]\n",
    "    contain = []\n",
    "    #Phan label cho cac client\n",
    "    for i in range(num_partitions):\n",
    "        current = [i%num_labels]\n",
    "        times[i%num_labels] += 1\n",
    "        if num_labels_each_party > 1:\n",
    "            current.append(1-i%num_labels)\n",
    "            times[1-i%num_labels] += 1\n",
    "        contain.append(current)\n",
    "    print(times)\n",
    "    print(contain)\n",
    "    # Create Subsets for each partition\n",
    "\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    for i in range(num_labels):\n",
    "        idx_i = np.where(train_labels == i)[0]  # Get indices of label i in train_labels\n",
    "        idx_i = [trainset.indices[j] for j in idx_i]  # Convert indices to indices in trainset\n",
    "        # #print label of idx_i\n",
    "        # print(\"Label of idx: \", i)\n",
    "        # for j in range(len(idx_i)):\n",
    "        #     idx_in_dataset = trainset.indices[idx_i[j]]\n",
    "        #     print(trainset.dataset.targets[idx_in_dataset])\n",
    "        np.random.shuffle(idx_i)\n",
    "        split = np.array_split(idx_i, times[i])\n",
    "        ids = 0\n",
    "        for j in range(num_partitions):\n",
    "            if i in contain[j]:\n",
    "                partition_indices[j].extend(split[ids])\n",
    "                ids += 1\n",
    "    \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    # #print label of client 0\n",
    "    # print(\"Client 0\")\n",
    "    # for i in range(len(trainsets[0])):\n",
    "    #     print(trainsets[0][i][1])\n",
    "\n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_imbalance_label_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 0.5, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    min_size = 0\n",
    "    min_require_size = 2\n",
    "\n",
    "    N = len(trainset)\n",
    "\n",
    "\n",
    "    while(min_size < min_require_size):\n",
    "        partition_indices = [[] for _ in range(num_partitions)]\n",
    "        for label in range(num_labels):\n",
    "            idx_label = np.where(train_labels == label)[0]\n",
    "            idx_label = [trainset.indices[j] for j in idx_label]\n",
    "            np.random.shuffle(idx_label)\n",
    "\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "            # proportions = np.array( [p * len(idx_j) < N/num_partitions] for p, idx_j in zip(proportions, partition_indices))\n",
    "            proportions = np.array([p if p * len(idx_j) < N / num_partitions else 0 for p, idx_j in zip(proportions, partition_indices)])\n",
    "\n",
    "            proportions = proportions / np.sum(proportions)\n",
    "            proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "\n",
    "            partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, np.split(idx_label, proportions))]\n",
    "            min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "\n",
    "def apply_gaussian_noise(tensor, std_dev):\n",
    "    noise = torch.randn_like(tensor) * std_dev\n",
    "    return tensor + noise\n",
    "\n",
    "# Hàm đảo ngược chuẩn hóa\n",
    "def unnormalize_image(image_tensor, mean, std):\n",
    "    # Đảo ngược Normalize: (image * std) + mean\n",
    "    for t, m, s in zip(image_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Thực hiện từng kênh\n",
    "    return image_tensor\n",
    "\n",
    "# Hàm hiển thị ảnh từ một tensor\n",
    "def display_image(image_tensor, mean, std):\n",
    "    # Đảo ngược chuẩn hóa\n",
    "    image_tensor = unnormalize_image(image_tensor, mean, std)\n",
    "    # Chuyển tensor thành NumPy array và điều chỉnh thứ tự kênh màu (CHW -> HWC)\n",
    "    image_numpy = image_tensor.permute(1, 2, 0).numpy()\n",
    "    # Cắt giá trị ảnh về phạm vi [0, 1] để hiển thị đúng\n",
    "    image_numpy = image_numpy.clip(0, 1)\n",
    "    # Trả về ảnh NumPy\n",
    "    return image_numpy\n",
    "\n",
    "\n",
    "\n",
    "def prepare_noise_based_imbalance(num_partitions: int, batch_size: int, val_ratio: float = 0.1, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Chia dữ liệu ngẫu nhiên và đều cho các bên, sau đó thêm noise vào các bên.\n",
    "    Mỗi bên i có noise khác nhau Gauss(0, sigma*i/N). Nếu dữ liệu đã tồn tại, tải từ thư mục dataset_noise_{sigma}.\n",
    "    \"\"\"\n",
    "    # noise_dir = f'chest_xray_noise_{sigma}'\n",
    "    # noise_dir = f'/kaggle/input/chest-xray-noise-60-partitions/chest_xray_noise_{sigma}'\n",
    "    noise_dir = f\"{BASE_FOLDER_NOISE}/chest_xray_noise_{sigma}\"\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    if os.path.exists(noise_dir):\n",
    "        print(f\"Loading noisy dataset from {noise_dir}...\")\n",
    "        # Sử dụng ImageFolder để tải dữ liệu đã được thêm nhiễu với transform phù hợp\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) for i in range(num_partitions)]\n",
    "        \n",
    "        # Tải val và test set như bình thường\n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "        print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "    else:\n",
    "        print(f\"Creating noisy dataset and saving to {noise_dir}...\")\n",
    "        os.makedirs(noise_dir, exist_ok=True)\n",
    "        \n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "        indices = trainset.indices\n",
    "        np.random.shuffle(indices)\n",
    "        partition_indices = np.array_split(indices, num_partitions)\n",
    "    \n",
    "        # Mean và std từ Normalize đã được định nghĩa trước\n",
    "        for i, part_indices in enumerate(partition_indices):\n",
    "            partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "            partition_set = Subset(trainset.dataset, part_indices)\n",
    "            \n",
    "            # Tạo thư mục cho partition và các lớp\n",
    "            partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "            os.makedirs(partition_dir, exist_ok=True)\n",
    "            class_dirs = {}\n",
    "            for _, label in partition_set:\n",
    "                if label not in class_dirs:\n",
    "                    class_dirs[label] = os.path.join(partition_dir, f'class_{label}')\n",
    "                    os.makedirs(class_dirs[label], exist_ok=True)\n",
    "            \n",
    "            for j, (image, label) in enumerate(partition_set):\n",
    "                noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "                # Đảo ngược chuẩn hóa để lưu ảnh đúng định dạng\n",
    "                noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "                # Chuyển tensor thành PIL Image\n",
    "                noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "                # Lưu ảnh với tên duy nhất\n",
    "                image_filename = f'image_{j}.png'\n",
    "                noisy_image_pil.save(os.path.join(class_dirs[label], image_filename))\n",
    "        \n",
    "        # Tải dữ liệu từ thư mục đã lưu với transform phù hợp\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) for i in range(num_partitions)]\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "    \n",
    "    # Phân tích phân bố lớp\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i].get(0, 0) for i in partitions]\n",
    "    class_1_counts = [class_distributions[i].get(1, 0) for i in partitions]\n",
    "    \n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Lưu ảnh nhiễu vào running_outputs\n",
    "    # Tạo thư mục lưu ảnh nếu chưa tồn tại\n",
    "    output_dir = \"running_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Khởi tạo một lưới 10x6 để hiển thị ảnh\n",
    "    fig, axes = plt.subplots(10, 6, figsize=(15, 25))\n",
    "    \n",
    "    # Duyệt qua trainloaders và hiển thị ảnh đầu tiên từ mỗi partition\n",
    "    for i, trainloader in enumerate(trainloaders[:min(num_partitions, 60)]):\n",
    "        if len(trainloader.dataset) == 0:\n",
    "            continue\n",
    "        # Lấy ảnh đầu tiên từ trainloader\n",
    "        image_tensor, label = trainloader.dataset[0]\n",
    "        \n",
    "        # Tìm vị trí hàng, cột trong lưới\n",
    "        row, col = divmod(i, 6)\n",
    "        if row >= 10:\n",
    "            break  # Chỉ hiển thị tối đa 60 ảnh\n",
    "        # Hiển thị ảnh\n",
    "        image_numpy = unnormalize_image(image_tensor.clone(), mean, std).permute(1, 2, 0).numpy().clip(0, 1)\n",
    "        axes[row, col].imshow(image_numpy)\n",
    "        axes[row, col].axis('off')\n",
    "    # Điều chỉnh layout để không bị chồng lấn\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Lưu ảnh thay vì hiển thị\n",
    "    output_path = os.path.join(output_dir, \"image_noise.png\")\n",
    "    plt.savefig(output_path, dpi=300)  # Lưu ảnh với chất lượng cao\n",
    "    \n",
    "    plt.close()  # Đóng figure\n",
    "    \n",
    "    print(f\"Ảnh minh họa đã được lưu tại {output_path}\")\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    \n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "def prepare_quantity_skew_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 10, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    all_indices = trainset.indices\n",
    "\n",
    "    min_size = 0\n",
    "    while min_size < 1:\n",
    "        proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(all_indices)).astype(int)[:-1]\n",
    "\n",
    "        partition_indices = np.split(all_indices, proportions)\n",
    "\n",
    "        min_size = min([len(partition) for partition in partition_indices])\n",
    "        print('Partition sizes:', [len(partition) for partition in partition_indices])\n",
    "        print('Min partition size:', min_size)\n",
    "\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    config: DictConfig,\n",
    "    num_clients: int,\n",
    "    val_ratio: float = 0.1,\n",
    "    seed: Optional[int] = 42,\n",
    ") -> Tuple[List[DataLoader], List[DataLoader], DataLoader]:\n",
    "    \"\"\"Create the dataloaders to be fed into the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: DictConfig\n",
    "        Parameterises the dataset partitioning process\n",
    "    num_clients : int\n",
    "        The number of clients that hold a part of the data\n",
    "    val_ratio : float, optional\n",
    "        The ratio of training data that will be used for validation (between 0 and 1),\n",
    "        by default 0.1\n",
    "    seed : int, optional\n",
    "        Used to set a fix seed to replicate experiments, by default 42\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[DataLoader, DataLoader, DataLoader]\n",
    "        The DataLoaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    print(f\"Dataset partitioning config: {config}\")\n",
    "    batch_size = -1\n",
    "    print('config:' , config)\n",
    "    if \"batch_size\" in config:\n",
    "        batch_size = config.batch_size\n",
    "    elif \"batch_size_ratio\" in config:\n",
    "        batch_size_ratio = config.batch_size_ratio\n",
    "    else:\n",
    "        raise ValueError\n",
    "    partitioning = \"\"\n",
    "    \n",
    "    if \"partitioning\" in config:\n",
    "        partitioning = config.partitioning\n",
    "\n",
    "    # partition the data\n",
    "    if partitioning == \"imbalance_label\":\n",
    "        return prepare_partitioned_dataset(num_clients, batch_size, val_ratio, config.labels_per_client, config.seed)\n",
    "\n",
    "    if partitioning == \"imbalance_label_dirichlet\":\n",
    "        return prepare_imbalance_label_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "\n",
    "    if partitioning == \"noise_based_imbalance\":\n",
    "        return prepare_noise_based_imbalance(num_clients, batch_size, val_ratio, config.sigma, config.seed)\n",
    "\n",
    "    if partitioning == \"quantity_skew_dirichlet\":\n",
    "        return prepare_quantity_skew_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_noise_based_imbalance(4, 10, 0, 0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imbalanced_and_noisy_data(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 0.5, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"Combine label-imbalanced partitioning and Gaussian noise application.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels (Dirichlet distribution)\n",
    "    num_labels = len(np.unique(train_labels))\n",
    "    min_size = 0\n",
    "    min_require_size = 2\n",
    "    N = len(trainset)\n",
    "\n",
    "    while min_size < min_require_size:\n",
    "        partition_indices = [[] for _ in range(num_partitions)]\n",
    "        for label in range(num_labels):\n",
    "            idx_label = np.where(train_labels == label)[0]\n",
    "            idx_label = [trainset.indices[j] for j in idx_label]\n",
    "            np.random.shuffle(idx_label)\n",
    "\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "            proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "\n",
    "            partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, np.split(idx_label, proportions))]\n",
    "        min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    # Add Gaussian noise to each partition\n",
    "    noisy_partitions = []\n",
    "    for i, train_partition in enumerate(trainsets):\n",
    "        partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "        noisy_data = []\n",
    "        for image, label in train_partition:\n",
    "            noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "            noisy_data.append((noisy_image, label))\n",
    "        noisy_partitions.append(noisy_data)\n",
    "\n",
    "    # Partition validation set equally\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Create DataLoaders\n",
    "    trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) for part in noisy_partitions]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "    # Analyze class distributions\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "\n",
    "    # Plot class distributions\n",
    "    partitions = range(num_partitions)\n",
    "    class_counts = [{cls: class_distributions[i].get(cls, 0) for cls in range(num_labels)} for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for cls in range(num_labels):\n",
    "        counts = [class_counts[i].get(cls, 0) for i in partitions]\n",
    "        plt.bar(partitions, counts, bar_width, label=f'Class {cls}')\n",
    "\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition_combined.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_imbalanced_and_noisy_data(4, 10, 0, 0.5, 0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_combined_imbalance(4, 10, 0, 0.5, 0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_imbalanced_and_noisy_data(num_partitions: int, batch_size: int, \n",
    "#                                       val_ratio: float = 0.1, beta: float = 0.5, \n",
    "#                                       sigma: float = 0.05, seed: int = 42):\n",
    "#     \"\"\"\n",
    "#     Phân chia dữ liệu với phân phối không cân bằng và thêm nhiễu Gaussian.\n",
    "#     Nếu dữ liệu đã được lưu, tải từ thư mục lưu trữ; nếu chưa, thực hiện phân chia và lưu.\n",
    "#     \"\"\"\n",
    "#     # Định nghĩa thư mục lưu trữ dựa trên các tham số\n",
    "#     noise_dir = f\"data_partition_combined_{num_partitions}_beta_{beta}_sigma_{sigma}\"\n",
    "#     mean = [0.485, 0.456, 0.406]\n",
    "#     std = [0.229, 0.224, 0.225]\n",
    "#     noisy_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std)\n",
    "#     ])\n",
    "    \n",
    "#     if os.path.exists(noise_dir):\n",
    "#         print(f\"Loading partitioned and noisy dataset from {noise_dir}...\")\n",
    "#         # Tải các partition đã lưu bằng ImageFolder\n",
    "#         train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "#                             for i in range(num_partitions)]\n",
    "        \n",
    "#         # Tải val và test set như bình thường\n",
    "#         trainset, testset = get_custom_dataset()\n",
    "#         num_train = int((1 - val_ratio) * len(trainset))\n",
    "#         num_val = len(trainset) - num_train\n",
    "#         _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "#         # Chia valset thành các partition\n",
    "#         partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "#         for i in range(len(valset) % num_partitions):\n",
    "#             partition_len_val[i] += 1\n",
    "#         valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "#         # Tạo DataLoaders\n",
    "#         trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "#                        for part in train_partitions]\n",
    "#         valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "#                       for vs in valsets]\n",
    "#         testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "#         print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "#     else:\n",
    "#         print(f\"Creating partitioned and noisy dataset and saving to {noise_dir}...\")\n",
    "#         os.makedirs(noise_dir, exist_ok=True)\n",
    "        \n",
    "#         trainset, testset = get_custom_dataset()\n",
    "#         num_train = int((1 - val_ratio) * len(trainset))\n",
    "#         num_val = len(trainset) - num_train\n",
    "#         train_subset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "#         # Lấy nhãn của toàn bộ trainset\n",
    "#         train_labels = np.array([train_subset.dataset.targets[i] for i in train_subset.indices])\n",
    "#         num_labels = len(np.unique(train_labels))\n",
    "#         min_size = 0\n",
    "#         min_require_size = 2\n",
    "#         N = len(train_subset)\n",
    "        \n",
    "#         # Phân chia dữ liệu theo phân phối Dirichlet\n",
    "#         while min_size < min_require_size:\n",
    "#             partition_indices = [[] for _ in range(num_partitions)]\n",
    "#             for label in range(num_labels):\n",
    "#                 idx_label = np.where(train_labels == label)[0]\n",
    "#                 idx_label = [train_subset.indices[j] for j in idx_label]\n",
    "#                 np.random.shuffle(idx_label)\n",
    "                \n",
    "#                 proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "#                 proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "                \n",
    "#                 splits = np.split(idx_label, proportions)\n",
    "#                 partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, splits)]\n",
    "#             min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "#         # Lưu các partition với nhiễu Gaussian\n",
    "#         for i, indices in enumerate(partition_indices):\n",
    "#             partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "#             os.makedirs(partition_dir, exist_ok=True)\n",
    "#             class_dirs = {}\n",
    "#             for idx in indices:\n",
    "#                 _, label = train_subset.dataset[idx]\n",
    "#                 if label not in class_dirs:\n",
    "#                     class_dirs[label] = os.path.join(partition_dir, f'class_{label}')\n",
    "#                     os.makedirs(class_dirs[label], exist_ok=True)\n",
    "            \n",
    "#             # Thêm nhiễu và lưu ảnh\n",
    "#             partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "#             for j, idx in enumerate(indices):\n",
    "#                 image, label = train_subset.dataset[idx]\n",
    "#                 noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "#                 noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "#                 noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "#                 image_filename = f'image_{j}.png'\n",
    "#                 noisy_image_pil.save(os.path.join(class_dirs[label], image_filename))\n",
    "        \n",
    "#         # Tải các partition đã lưu bằng ImageFolder\n",
    "#         train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "#                             for i in range(num_partitions)]\n",
    "        \n",
    "#         # Chia valset thành các partition\n",
    "#         partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "#         for i in range(len(valset) % num_partitions):\n",
    "#             partition_len_val[i] += 1\n",
    "#         valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "#         # Tạo DataLoaders\n",
    "#         trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "#                        for part in train_partitions]\n",
    "#         valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "#                       for vs in valsets]\n",
    "#         testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "#         print(f\"Dữ liệu đã được phân chia và lưu tại {noise_dir}\")\n",
    "    \n",
    "#     # Phân tích phân bố lớp\n",
    "#     class_distributions = []\n",
    "#     for i, trainloader in enumerate(trainloaders):\n",
    "#         class_counts = Counter()\n",
    "#         for _, labels in trainloader:\n",
    "#             class_counts.update(labels.numpy())\n",
    "#         class_distributions.append(class_counts)\n",
    "#         print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "#     # Vẽ biểu đồ phân bố lớp\n",
    "#     partitions = range(num_partitions)\n",
    "#     class_counts = [{cls: class_distributions[i].get(cls, 0) for cls in range(num_labels)} for i in partitions]\n",
    "    \n",
    "#     bar_width = 0.5\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     for cls in range(num_labels):\n",
    "#         counts = [class_counts[i].get(cls, 0) for i in partitions]\n",
    "#         plt.bar(partitions, counts, bar_width, label=f'Class {cls}')\n",
    "    \n",
    "#     plt.xlabel('Partition')\n",
    "#     plt.ylabel('Number of Samples')\n",
    "#     plt.title('Class Distribution in Each Partition')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     output_dir = 'running_outputs'\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     plt.savefig(os.path.join(output_dir, 'data_partition_combined.png'))\n",
    "#     plt.close()\n",
    "    \n",
    "#     print(f'Number of train samples: {len(train_subset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    \n",
    "#     return trainloaders, valloaders, testloader\n",
    "def prepare_imbalanced_and_noisy_data(num_partitions: int, batch_size: int, \n",
    "                                      val_ratio: float = 0.1, beta: float = 0.5, \n",
    "                                      sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Phân chia dữ liệu với phân phối không cân bằng và thêm nhiễu Gaussian.\n",
    "    Nếu dữ liệu đã được lưu, tải từ thư mục lưu trữ; nếu chưa, thực hiện phân chia và lưu.\n",
    "    \"\"\"\n",
    "    # Định nghĩa thư mục lưu trữ dựa trên các tham số\n",
    "    # noise_dir = f\"data_partition_combined_{num_partitions}_beta_{beta}_sigma_{sigma}\"\n",
    "    noise_dir = f\"{BASE_FOLDER_NOISE}/chest_xray_noise_drl_label{beta}_{sigma}\"\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    if os.path.exists(noise_dir):\n",
    "        print(f\"Loading partitioned and noisy dataset from {noise_dir}...\")\n",
    "        # Tải các partition đã lưu bằng ImageFolder\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "                            for i in range(num_partitions)]\n",
    "        \n",
    "        # Lấy số lượng lớp từ một trong các partition\n",
    "        if len(train_partitions) > 0:\n",
    "            num_labels = len(train_partitions[0].classes)\n",
    "        else:\n",
    "            raise ValueError(\"Không tìm thấy partition nào trong thư mục lưu trữ.\")\n",
    "        \n",
    "        # Tải val và test set như bình thường\n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "                       for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "                      for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "        print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "    else:\n",
    "        print(f\"Creating partitioned and noisy dataset and saving to {noise_dir}...\")\n",
    "        os.makedirs(noise_dir, exist_ok=True)\n",
    "        \n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        train_subset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Lấy nhãn của toàn bộ trainset\n",
    "        train_labels = np.array([train_subset.dataset.targets[i] for i in train_subset.indices])\n",
    "        num_labels = len(np.unique(train_labels))\n",
    "        min_size = 0\n",
    "        min_require_size = 2\n",
    "        N = len(train_subset)\n",
    "        \n",
    "        # Phân chia dữ liệu theo phân phối Dirichlet\n",
    "        while min_size < min_require_size:\n",
    "            partition_indices = [[] for _ in range(num_partitions)]\n",
    "            for label in range(num_labels):\n",
    "                idx_label = np.where(train_labels == label)[0]\n",
    "                idx_label = [train_subset.indices[j] for j in idx_label]\n",
    "                np.random.shuffle(idx_label)\n",
    "                \n",
    "                proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "                proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "                \n",
    "                splits = np.split(idx_label, proportions)\n",
    "                partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, splits)]\n",
    "            min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "        # Lưu các partition với nhiễu Gaussian\n",
    "        for i, indices in enumerate(partition_indices):\n",
    "            partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "            os.makedirs(partition_dir, exist_ok=True)\n",
    "            class_dirs = {}\n",
    "            for idx in indices:\n",
    "                _, label = train_subset.dataset[idx]\n",
    "                if label not in class_dirs:\n",
    "                    class_dirs[label] = os.path.join(partition_dir, f'class_{label}')\n",
    "                    os.makedirs(class_dirs[label], exist_ok=True)\n",
    "            \n",
    "            # Thêm nhiễu và lưu ảnh\n",
    "            partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "            for j, idx in enumerate(indices):\n",
    "                image, label = train_subset.dataset[idx]\n",
    "                noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "                noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "                noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "                image_filename = f'image_{j}.png'\n",
    "                noisy_image_pil.save(os.path.join(class_dirs[label], image_filename))\n",
    "        \n",
    "        # Tải các partition đã lưu bằng ImageFolder\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "                            for i in range(num_partitions)]\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "                       for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "                      for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "        print(f\"Dữ liệu đã được phân chia và lưu tại {noise_dir}\")\n",
    "    \n",
    "    # Phân tích phân bố lớp\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    # Vẽ biểu đồ phân bố lớp\n",
    "    partitions = range(num_partitions)\n",
    "    class_counts_list = []\n",
    "    for i in partitions:\n",
    "        counts = {cls: class_distributions[i].get(cls, 0) for cls in range(num_labels)}\n",
    "        class_counts_list.append(counts)\n",
    "    \n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bottom = np.zeros(num_partitions)\n",
    "    colors = plt.cm.tab10.colors  # Sử dụng bảng màu có sẵn\n",
    "    \n",
    "    for cls in range(num_labels):\n",
    "        counts = [class_counts_list[i].get(cls, 0) for i in partitions]\n",
    "        plt.bar(partitions, counts, bar_width, bottom=bottom, label=f'Class {cls}', color=colors[cls % len(colors)])\n",
    "        bottom += counts\n",
    "    \n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition_combined.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f'Number of train samples: {sum(len(loader.dataset) for loader in trainloaders)}, '\n",
    "          f'val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    \n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_imbalanced_and_noisy_data(4, 10, 0, 0.5, 1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_quantity_drl_and_noisy_data(num_partitions: int, batch_size: int, \n",
    "                                      val_ratio: float = 0.1, beta: float = 10, \n",
    "                                      sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Phân chia dữ liệu với phân phối không cân bằng theo quantity_skew_dirichlet và thêm nhiễu Gaussian.\n",
    "    \"\"\"\n",
    "    noise_dir = f\"{BASE_FOLDER_NOISE}/chest_xray_noise_drl_quantity_skew_{beta}_{sigma}\"\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    if os.path.exists(noise_dir):\n",
    "        print(f\"Loading partitioned and noisy dataset from {noise_dir}...\")\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "                            for i in range(num_partitions)]\n",
    "        \n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "                       for part in train_partitions]\n",
    "        \n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "                      for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "        class_distributions = []\n",
    "        for i, trainloader in enumerate(trainloaders):\n",
    "            class_counts = Counter()\n",
    "            for _, labels in trainloader:\n",
    "                class_counts.update(labels.numpy())\n",
    "            class_distributions.append(class_counts)\n",
    "            print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "        \n",
    "        partitions = range(num_partitions)\n",
    "        class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "        class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "        bar_width = 0.5\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "        plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "        plt.xlabel('Partition')\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.title('Class Distribution in Each Partition')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        # plt.show()\n",
    "        #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "        output_dir = 'running_outputs'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "        plt.close()\n",
    "\n",
    "        print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "        print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "    else:\n",
    "        print(f\"Creating partitioned and noisy dataset and saving to {noise_dir}...\")\n",
    "        os.makedirs(noise_dir, exist_ok=True)\n",
    "        \n",
    "        trainloaders, valloaders, testloader = prepare_quantity_skew_dirichlet(\n",
    "            num_partitions=num_partitions, batch_size=batch_size, val_ratio=val_ratio, beta=beta, seed=seed)\n",
    "        \n",
    "        # Thêm nhiễu Gaussian và lưu dữ liệu\n",
    "        for i, trainloader in enumerate(trainloaders):\n",
    "            partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "            os.makedirs(partition_dir, exist_ok=True)\n",
    "            for images, labels in trainloader:\n",
    "                for j, (image, label) in enumerate(zip(images, labels)):\n",
    "                    class_dir = os.path.join(partition_dir, f'class_{label.item()}')\n",
    "                    os.makedirs(class_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Thêm nhiễu Gaussian\n",
    "                    partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "                    noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "                    noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "                    noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "                    image_filename = f'image_{j}.png'\n",
    "                    noisy_image_pil.save(os.path.join(class_dir, image_filename))\n",
    "        \n",
    "        print(f\"Dữ liệu đã được phân chia và lưu tại {noise_dir}\")\n",
    "\n",
    "       \n",
    "    return trainloaders, valloaders, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading partitioned and noisy dataset from /media/namvq/Data/code_chinh_sua/fedavg/chest_xray_noise_drl_quantity_skew_0.5_0.1...\n",
      "Partition 0 class distribution: {0: 10, 1: 10}\n",
      "Partition 1 class distribution: {1: 7, 0: 3}\n",
      "Partition 2 class distribution: {0: 10, 1: 10}\n",
      "Partition 3 class distribution: {0: 2}\n",
      "Partition 4 class distribution: {1: 1, 0: 1}\n",
      "Partition 5 class distribution: {1: 10, 0: 7}\n",
      "Partition 6 class distribution: {0: 9, 1: 10}\n",
      "Partition 7 class distribution: {0: 7}\n",
      "Partition 8 class distribution: {0: 10, 1: 10}\n",
      "Partition 9 class distribution: {1: 10, 0: 10}\n",
      "Partition 10 class distribution: {1: 8, 0: 6}\n",
      "Partition 11 class distribution: {0: 10, 1: 10}\n",
      "Partition 12 class distribution: {0: 4}\n",
      "Partition 13 class distribution: {0: 8, 1: 10}\n",
      "Partition 14 class distribution: {1: 10, 0: 7}\n",
      "Partition 15 class distribution: {1: 10, 0: 10}\n",
      "Partition 16 class distribution: {1: 10, 0: 8}\n",
      "Partition 17 class distribution: {1: 10, 0: 6}\n",
      "Partition 18 class distribution: {0: 2, 1: 3}\n",
      "Partition 19 class distribution: {0: 9, 1: 10}\n",
      "Partition 20 class distribution: {1: 10, 0: 9}\n",
      "Partition 21 class distribution: {0: 2}\n",
      "Partition 22 class distribution: {0: 2}\n",
      "Partition 23 class distribution: {0: 9, 1: 10}\n",
      "Partition 24 class distribution: {1: 9, 0: 6}\n",
      "Partition 25 class distribution: {0: 1}\n",
      "Partition 26 class distribution: {1: 8, 0: 2}\n",
      "Partition 27 class distribution: {0: 1}\n",
      "Partition 28 class distribution: {0: 10, 1: 10}\n",
      "Partition 29 class distribution: {1: 9, 0: 2}\n",
      "Partition 30 class distribution: {0: 9, 1: 10}\n",
      "Partition 31 class distribution: {1: 4, 0: 2}\n",
      "Partition 32 class distribution: {1: 10, 0: 10}\n",
      "Partition 33 class distribution: {1: 10, 0: 3}\n",
      "Partition 34 class distribution: {0: 1}\n",
      "Partition 35 class distribution: {1: 10, 0: 9}\n",
      "Partition 36 class distribution: {0: 10, 1: 10}\n",
      "Partition 37 class distribution: {1: 10, 0: 10}\n",
      "Partition 38 class distribution: {0: 5}\n",
      "Partition 39 class distribution: {0: 10, 1: 10}\n",
      "Partition 40 class distribution: {1: 10, 0: 6}\n",
      "Partition 41 class distribution: {0: 5, 1: 8}\n",
      "Partition 42 class distribution: {0: 10, 1: 10}\n",
      "Partition 43 class distribution: {1: 10, 0: 10}\n",
      "Partition 44 class distribution: {1: 10, 0: 6}\n",
      "Partition 45 class distribution: {1: 10, 0: 10}\n",
      "Partition 46 class distribution: {1: 2, 0: 1}\n",
      "Partition 47 class distribution: {0: 6, 1: 10}\n",
      "Partition 48 class distribution: {1: 9, 0: 1}\n",
      "Partition 49 class distribution: {0: 9, 1: 10}\n",
      "Partition 50 class distribution: {1: 6, 0: 1}\n",
      "Partition 51 class distribution: {1: 10, 0: 7}\n",
      "Partition 52 class distribution: {1: 3, 0: 1}\n",
      "Partition 53 class distribution: {1: 10, 0: 10}\n",
      "Partition 54 class distribution: {1: 9, 0: 5}\n",
      "Partition 55 class distribution: {0: 5, 1: 9}\n",
      "Partition 56 class distribution: {0: 8, 1: 10}\n",
      "Partition 57 class distribution: {0: 10, 1: 10}\n",
      "Partition 58 class distribution: {0: 10, 1: 10}\n",
      "Partition 59 class distribution: {1: 10, 0: 10}\n",
      "Number of train samples: 5216, val samples: 0, test samples: 129\n",
      "Dữ liệu đã được tải thành công từ thư mục lưu trữ.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<torch.utils.data.dataloader.DataLoader at 0x7dde47188100>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471881c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188280>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188370>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188460>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188550>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188640>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188730>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188820>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188910>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188a00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188af0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188be0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188cd0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188dc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188eb0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47188fa0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189090>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189180>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189270>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189360>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189450>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189540>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189630>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189720>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189810>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189900>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471899f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189ae0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189bd0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189cc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189db0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189ea0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47189f90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a080>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a170>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a260>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a350>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a440>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a530>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a620>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a710>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a800>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a8f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718a9e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718aad0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718abc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718acb0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718ada0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718ae90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718af80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718b070>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718b160>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718b250>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718b340>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718b430>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718b520>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718b610>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718b700>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718b7f0>],\n",
       " [<torch.utils.data.dataloader.DataLoader at 0x7dde4718bca0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde4718bcd0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9540>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9630>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9720>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9810>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9900>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f99f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9ae0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9bd0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9cc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9db0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9ea0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471f9f90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa080>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa170>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa260>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa350>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa440>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa530>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa620>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa710>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa800>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa8f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fa9e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471faad0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fabc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471facb0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fada0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fae90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471faf80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb070>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb160>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb250>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb340>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb430>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb520>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb610>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb700>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb7f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb8e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fb9d0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fbac0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fbbb0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fbca0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fbd90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fbe80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde471fbf70>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde472040a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204190>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204280>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204370>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204460>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204550>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204640>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204730>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204820>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204910>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204a00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47204af0>],\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7ddecde1aec0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_quantity_drl_and_noisy_data(60, 10, 0, 0.5, 0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating partitioned and noisy dataset and saving to /media/namvq/Data/code_chinh_sua/fedavg/chest_xray_noise_drl_quantity_skew_0.5_0.3...\n",
      "Partition sizes: [111, 400, 210, 0, 151, 59, 2, 162, 3, 382, 46, 102, 2, 7, 8, 6, 4, 431, 9, 1, 2, 241, 119, 43, 30, 112, 8, 12, 1, 9, 29, 29, 24, 2, 106, 110, 0, 70, 134, 86, 28, 121, 154, 6, 420, 1, 15, 7, 233, 10, 175, 2, 190, 0, 112, 249, 164, 24, 20, 22]\n",
      "Min partition size: 0\n",
      "Partition sizes: [4, 1, 20, 80, 92, 228, 39, 172, 3, 389, 229, 5, 53, 1, 421, 6, 5, 49, 9, 1, 0, 22, 51, 57, 267, 1, 1, 407, 167, 13, 101, 49, 64, 24, 14, 10, 1, 208, 45, 58, 0, 159, 57, 0, 28, 160, 32, 27, 38, 11, 21, 11, 22, 21, 234, 36, 30, 141, 6, 815]\n",
      "Min partition size: 0\n",
      "Partition sizes: [66, 73, 5, 0, 166, 28, 28, 13, 94, 64, 176, 6, 328, 145, 4, 33, 23, 47, 9, 9, 305, 323, 2, 92, 410, 95, 5, 570, 42, 0, 2, 0, 95, 38, 10, 175, 30, 38, 10, 20, 13, 549, 15, 20, 282, 6, 0, 19, 31, 42, 1, 4, 85, 62, 334, 2, 59, 100, 12, 1]\n",
      "Min partition size: 0\n",
      "Partition sizes: [20, 171, 26, 44, 9, 9, 65, 35, 15, 0, 1, 8, 0, 172, 19, 24, 18, 41, 269, 39, 184, 174, 1, 4, 28, 83, 220, 86, 259, 362, 4, 4, 23, 124, 17, 242, 185, 0, 8, 37, 1, 142, 312, 1, 295, 359, 61, 437, 26, 1, 124, 47, 0, 13, 95, 52, 3, 39, 17, 161]\n",
      "Min partition size: 0\n",
      "Partition sizes: [298, 120, 42, 158, 45, 25, 25, 65, 192, 3, 5, 58, 19, 69, 26, 1, 264, 215, 1, 6, 114, 5, 0, 65, 16, 31, 6, 4, 139, 303, 171, 24, 219, 1, 170, 112, 223, 109, 17, 0, 1, 52, 0, 3, 93, 94, 623, 23, 1, 351, 49, 2, 133, 4, 55, 11, 55, 15, 22, 263]\n",
      "Min partition size: 0\n",
      "Partition sizes: [0, 8, 108, 63, 46, 8, 3, 6, 3, 82, 295, 98, 319, 252, 71, 50, 5, 213, 51, 4, 16, 10, 71, 33, 39, 57, 2, 199, 89, 5, 46, 20, 68, 221, 61, 201, 58, 7, 112, 125, 50, 0, 131, 11, 0, 38, 2, 53, 19, 103, 38, 32, 185, 279, 762, 17, 92, 54, 8, 217]\n",
      "Min partition size: 0\n",
      "Partition sizes: [197, 7, 9, 57, 82, 32, 9, 3, 159, 0, 390, 66, 50, 0, 4, 12, 226, 166, 28, 0, 94, 34, 3, 132, 143, 7, 18, 165, 21, 92, 277, 105, 9, 252, 36, 129, 9, 87, 259, 1, 36, 322, 2, 10, 20, 194, 100, 172, 206, 1, 342, 1, 107, 32, 178, 7, 11, 6, 67, 32]\n",
      "Min partition size: 0\n",
      "Partition sizes: [28, 64, 18, 194, 373, 27, 313, 38, 64, 1, 9, 0, 7, 54, 9, 340, 64, 78, 180, 16, 2, 132, 0, 15, 16, 634, 105, 1, 22, 8, 159, 35, 258, 7, 15, 0, 190, 172, 193, 93, 60, 22, 118, 50, 8, 264, 4, 23, 76, 88, 169, 15, 37, 76, 0, 108, 20, 15, 14, 115]\n",
      "Min partition size: 0\n",
      "Partition sizes: [294, 251, 1, 58, 67, 679, 71, 94, 145, 47, 6, 129, 1, 145, 165, 65, 38, 52, 189, 2, 2, 126, 0, 218, 144, 1, 134, 0, 99, 127, 35, 43, 22, 9, 49, 3, 266, 2, 374, 121, 40, 20, 41, 0, 23, 51, 17, 1, 2, 201, 123, 0, 283, 13, 7, 26, 0, 48, 1, 45]\n",
      "Min partition size: 0\n",
      "Partition sizes: [192, 14, 24, 102, 108, 474, 64, 1, 54, 121, 67, 69, 89, 4, 185, 26, 3, 4, 8, 103, 24, 81, 9, 124, 0, 2, 102, 8, 13, 13, 381, 147, 30, 209, 34, 229, 10, 4, 88, 453, 2, 4, 298, 105, 55, 6, 21, 22, 143, 17, 87, 259, 55, 135, 15, 90, 7, 116, 8, 98]\n",
      "Min partition size: 0\n",
      "Partition sizes: [71, 5, 101, 111, 7, 65, 2, 37, 20, 165, 171, 213, 35, 151, 15, 78, 28, 410, 178, 1, 518, 298, 34, 60, 78, 1, 11, 28, 94, 81, 265, 0, 20, 58, 110, 5, 14, 2, 176, 1, 36, 0, 11, 112, 12, 303, 5, 16, 1, 6, 4, 26, 1, 284, 1, 1, 4, 0, 671, 4]\n",
      "Min partition size: 0\n",
      "Partition sizes: [42, 58, 64, 96, 178, 4, 86, 269, 119, 258, 3, 7, 0, 4, 5, 77, 92, 155, 74, 206, 6, 2, 174, 90, 39, 22, 5, 1, 1, 151, 271, 1, 59, 99, 117, 116, 271, 23, 6, 112, 1, 214, 44, 1, 1, 14, 99, 31, 3, 276, 15, 78, 211, 16, 309, 108, 363, 36, 2, 31]\n",
      "Min partition size: 0\n",
      "Partition sizes: [0, 23, 190, 61, 125, 10, 4, 111, 21, 0, 32, 16, 201, 80, 0, 5, 0, 106, 385, 89, 6, 13, 328, 3, 589, 70, 0, 312, 29, 1, 3, 11, 2, 94, 66, 74, 9, 33, 431, 2, 789, 43, 76, 0, 3, 56, 1, 27, 4, 199, 41, 33, 5, 128, 187, 58, 5, 1, 21, 4]\n",
      "Min partition size: 0\n",
      "Partition sizes: [21, 11, 2, 4, 144, 255, 117, 5, 89, 306, 1, 224, 32, 8, 250, 53, 26, 222, 461, 87, 45, 78, 84, 62, 5, 160, 107, 116, 7, 260, 111, 277, 22, 81, 12, 28, 14, 49, 242, 4, 0, 2, 65, 24, 158, 0, 275, 10, 57, 36, 2, 69, 285, 8, 18, 83, 4, 8, 5, 25]\n",
      "Min partition size: 0\n",
      "Partition sizes: [28, 79, 2, 0, 14, 27, 28, 35, 2, 2, 179, 7, 57, 177, 132, 53, 385, 1, 56, 37, 98, 8, 271, 20, 108, 6, 74, 141, 10, 64, 86, 116, 24, 1, 38, 134, 76, 313, 5, 60, 13, 96, 3, 81, 236, 40, 97, 661, 82, 217, 106, 9, 96, 16, 144, 21, 5, 35, 303, 1]\n",
      "Min partition size: 0\n",
      "Partition sizes: [11, 90, 57, 90, 162, 99, 2, 112, 3, 27, 40, 0, 7, 56, 265, 194, 101, 32, 305, 24, 0, 13, 297, 22, 0, 48, 193, 4, 69, 27, 92, 244, 100, 153, 51, 47, 14, 5, 56, 69, 1, 3, 65, 141, 36, 251, 98, 6, 188, 147, 1, 120, 0, 17, 15, 25, 342, 18, 475, 86]\n",
      "Min partition size: 0\n",
      "Partition sizes: [344, 33, 15, 4, 0, 223, 3, 710, 25, 1, 8, 222, 263, 213, 14, 3, 0, 130, 84, 124, 154, 26, 4, 23, 5, 5, 18, 16, 276, 13, 38, 35, 231, 93, 31, 39, 278, 111, 27, 19, 286, 15, 178, 18, 1, 10, 38, 39, 4, 9, 4, 6, 2, 10, 50, 106, 165, 79, 318, 17]\n",
      "Min partition size: 0\n",
      "Partition sizes: [145, 10, 121, 45, 40, 1, 36, 77, 18, 21, 31, 11, 49, 46, 1, 393, 97, 0, 40, 149, 2, 28, 150, 1, 441, 20, 0, 20, 342, 399, 97, 36, 33, 5, 1, 237, 10, 42, 140, 12, 10, 21, 13, 290, 1, 317, 16, 87, 91, 205, 71, 0, 155, 56, 134, 52, 15, 76, 235, 24]\n",
      "Min partition size: 0\n",
      "Partition sizes: [0, 233, 158, 2, 263, 55, 1, 95, 72, 6, 224, 13, 13, 80, 13, 298, 390, 70, 5, 45, 30, 33, 1, 39, 517, 16, 3, 8, 180, 52, 104, 3, 111, 39, 1, 71, 23, 83, 37, 39, 19, 183, 120, 99, 126, 16, 273, 2, 31, 7, 26, 5, 11, 8, 17, 36, 12, 690, 9, 100]\n",
      "Min partition size: 0\n",
      "Partition sizes: [155, 79, 36, 55, 12, 132, 38, 73, 119, 139, 279, 44, 3, 32, 702, 339, 128, 23, 1, 4, 28, 62, 3, 5, 119, 12, 33, 187, 19, 55, 133, 35, 232, 70, 134, 35, 54, 27, 111, 24, 91, 26, 327, 169, 2, 78, 16, 11, 183, 0, 3, 53, 18, 22, 7, 10, 147, 1, 34, 247]\n",
      "Min partition size: 0\n",
      "Partition sizes: [2, 31, 0, 14, 237, 69, 107, 127, 85, 184, 20, 140, 266, 13, 149, 7, 27, 27, 168, 13, 84, 5, 188, 68, 23, 34, 119, 98, 1, 0, 115, 13, 17, 34, 44, 198, 10, 302, 183, 14, 51, 584, 1, 7, 160, 8, 35, 45, 2, 37, 136, 185, 11, 70, 178, 0, 30, 48, 1, 391]\n",
      "Min partition size: 0\n",
      "Partition sizes: [213, 5, 4, 96, 0, 54, 274, 20, 501, 88, 8, 11, 64, 68, 0, 362, 12, 6, 11, 152, 104, 189, 34, 160, 0, 116, 10, 22, 26, 21, 0, 266, 220, 14, 25, 67, 103, 80, 256, 22, 23, 3, 2, 45, 33, 185, 137, 25, 67, 13, 221, 64, 21, 1, 2, 6, 54, 103, 524, 3]\n",
      "Min partition size: 0\n",
      "Partition sizes: [73, 3, 36, 1, 228, 222, 9, 57, 24, 113, 6, 141, 2, 12, 14, 159, 161, 15, 450, 65, 34, 134, 16, 110, 85, 98, 279, 26, 106, 130, 68, 0, 9, 56, 35, 75, 249, 117, 223, 29, 120, 13, 151, 275, 43, 241, 60, 3, 38, 0, 93, 59, 2, 0, 117, 128, 143, 9, 1, 50]\n",
      "Min partition size: 0\n",
      "Partition sizes: [34, 12, 53, 100, 116, 26, 19, 49, 10, 92, 6, 9, 33, 32, 27, 513, 21, 179, 0, 31, 27, 4, 114, 0, 8, 35, 36, 54, 112, 2, 16, 73, 570, 133, 362, 73, 2, 63, 32, 100, 48, 0, 13, 51, 0, 84, 384, 44, 170, 11, 7, 251, 14, 83, 22, 41, 15, 6, 438, 356]\n",
      "Min partition size: 0\n",
      "Partition sizes: [101, 44, 63, 1, 95, 91, 6, 76, 87, 34, 2, 123, 22, 156, 209, 103, 119, 3, 95, 80, 262, 120, 205, 197, 263, 219, 180, 3, 0, 141, 15, 77, 64, 0, 169, 15, 72, 8, 3, 62, 20, 0, 8, 34, 118, 225, 114, 20, 0, 53, 4, 476, 73, 11, 39, 24, 3, 396, 1, 12]\n",
      "Min partition size: 0\n",
      "Partition sizes: [13, 221, 117, 292, 79, 54, 78, 177, 110, 9, 17, 72, 54, 34, 103, 2, 319, 4, 46, 3, 30, 123, 13, 16, 28, 2, 0, 37, 9, 0, 236, 140, 117, 18, 123, 25, 35, 131, 3, 217, 17, 3, 145, 483, 123, 37, 8, 0, 73, 130, 20, 113, 272, 131, 176, 76, 236, 48, 10, 8]\n",
      "Min partition size: 0\n",
      "Partition sizes: [2, 115, 56, 81, 142, 130, 3, 41, 177, 4, 8, 14, 121, 19, 0, 53, 104, 21, 38, 7, 223, 159, 1, 25, 272, 19, 29, 48, 34, 89, 50, 1, 10, 1, 197, 183, 0, 6, 6, 110, 62, 253, 98, 361, 80, 116, 0, 67, 189, 28, 0, 16, 4, 193, 342, 250, 126, 87, 227, 118]\n",
      "Min partition size: 0\n",
      "Partition sizes: [36, 28, 115, 199, 66, 42, 76, 11, 136, 88, 16, 1, 46, 37, 26, 39, 5, 2, 19, 16, 10, 178, 136, 5, 93, 59, 14, 237, 35, 10, 73, 69, 8, 184, 107, 4, 43, 76, 0, 136, 345, 301, 418, 60, 178, 109, 62, 1, 0, 564, 4, 403, 4, 136, 31, 13, 10, 81, 9, 6]\n",
      "Min partition size: 0\n",
      "Partition sizes: [12, 1, 0, 26, 51, 13, 144, 175, 10, 75, 14, 443, 107, 38, 204, 6, 12, 50, 95, 46, 35, 57, 1, 2, 154, 24, 17, 4, 213, 227, 32, 1, 60, 8, 18, 4, 10, 1, 388, 213, 46, 0, 14, 212, 60, 5, 112, 2, 13, 23, 419, 503, 10, 278, 84, 355, 22, 53, 9, 5]\n",
      "Min partition size: 0\n",
      "Partition sizes: [523, 2, 9, 5, 143, 1, 77, 40, 60, 157, 1, 20, 89, 6, 25, 0, 23, 385, 64, 121, 840, 30, 0, 458, 152, 50, 26, 108, 3, 138, 72, 104, 32, 171, 34, 45, 47, 0, 18, 43, 32, 46, 42, 0, 6, 45, 0, 125, 0, 215, 182, 79, 18, 3, 116, 1, 0, 22, 68, 94]\n",
      "Min partition size: 0\n",
      "Partition sizes: [270, 0, 166, 146, 1, 50, 102, 10, 209, 6, 255, 23, 78, 0, 18, 23, 75, 217, 437, 130, 51, 100, 5, 1, 32, 76, 10, 7, 1, 252, 43, 16, 446, 3, 2, 74, 2, 237, 16, 150, 146, 31, 72, 115, 63, 216, 14, 48, 78, 6, 39, 23, 50, 43, 271, 35, 32, 40, 27, 127]\n",
      "Min partition size: 0\n",
      "Partition sizes: [0, 32, 10, 24, 21, 0, 66, 9, 208, 73, 26, 58, 0, 17, 56, 11, 364, 326, 42, 120, 2, 3, 8, 172, 188, 24, 213, 19, 25, 3, 12, 248, 1, 52, 651, 54, 81, 94, 86, 2, 1, 139, 82, 0, 88, 183, 6, 3, 4, 29, 113, 0, 236, 38, 82, 2, 280, 76, 415, 38]\n",
      "Min partition size: 0\n",
      "Partition sizes: [30, 87, 149, 18, 330, 53, 5, 62, 312, 158, 11, 111, 131, 10, 74, 101, 0, 2, 49, 21, 168, 43, 58, 48, 6, 0, 67, 238, 24, 42, 10, 27, 2, 87, 1, 68, 11, 14, 35, 28, 0, 25, 32, 502, 235, 317, 137, 327, 145, 110, 0, 17, 0, 46, 536, 5, 53, 5, 26, 7]\n",
      "Min partition size: 0\n",
      "Partition sizes: [20, 5, 29, 16, 29, 2, 45, 686, 100, 4, 20, 22, 110, 23, 744, 22, 120, 16, 30, 5, 0, 108, 9, 1, 206, 69, 47, 51, 97, 33, 77, 0, 0, 14, 8, 442, 48, 44, 2, 307, 30, 78, 117, 84, 88, 82, 154, 55, 13, 69, 4, 30, 1, 39, 73, 3, 39, 34, 378, 234]\n",
      "Min partition size: 0\n",
      "Partition sizes: [16, 1, 195, 319, 157, 12, 281, 7, 198, 75, 7, 47, 86, 1, 124, 49, 6, 1, 9, 234, 507, 0, 5, 92, 107, 4, 27, 34, 2, 44, 919, 48, 33, 198, 51, 12, 106, 52, 14, 8, 6, 84, 28, 7, 39, 7, 113, 7, 75, 51, 189, 181, 9, 67, 31, 10, 36, 125, 9, 54]\n",
      "Min partition size: 0\n",
      "Partition sizes: [698, 70, 191, 50, 6, 24, 15, 2, 2, 85, 2, 100, 277, 154, 38, 210, 28, 0, 18, 2, 137, 4, 0, 167, 150, 4, 20, 73, 6, 0, 33, 342, 210, 425, 45, 105, 70, 1, 13, 0, 28, 68, 228, 190, 77, 26, 1, 114, 36, 313, 40, 5, 0, 4, 53, 17, 29, 43, 48, 119]\n",
      "Min partition size: 0\n",
      "Partition sizes: [15, 5, 10, 13, 96, 130, 123, 447, 46, 10, 347, 4, 84, 170, 49, 252, 2, 18, 6, 60, 0, 185, 22, 47, 10, 384, 1, 0, 214, 29, 131, 154, 63, 5, 2, 1, 1, 9, 63, 318, 1, 162, 120, 41, 192, 1, 279, 85, 0, 9, 7, 73, 153, 161, 22, 113, 28, 108, 129, 6]\n",
      "Min partition size: 0\n",
      "Partition sizes: [84, 0, 73, 45, 113, 0, 147, 45, 0, 1, 143, 0, 37, 131, 113, 178, 13, 143, 318, 2, 233, 3, 7, 3, 0, 7, 52, 12, 38, 8, 35, 774, 57, 1, 0, 24, 2, 75, 40, 10, 30, 349, 92, 153, 131, 36, 0, 1, 370, 66, 17, 0, 5, 617, 107, 138, 112, 15, 9, 1]\n",
      "Min partition size: 0\n",
      "Partition sizes: [250, 56, 3, 16, 6, 169, 643, 126, 101, 559, 91, 202, 64, 3, 4, 365, 35, 35, 113, 0, 14, 8, 3, 39, 216, 0, 91, 2, 18, 18, 322, 51, 2, 43, 16, 267, 19, 51, 155, 20, 447, 24, 0, 1, 0, 1, 44, 9, 28, 0, 45, 38, 1, 42, 13, 68, 194, 49, 5, 11]\n",
      "Min partition size: 0\n",
      "Partition sizes: [57, 137, 106, 25, 353, 87, 71, 14, 73, 33, 13, 198, 100, 65, 110, 70, 114, 0, 34, 35, 133, 55, 11, 2, 373, 4, 151, 7, 175, 66, 2, 10, 1, 80, 120, 125, 0, 2, 132, 5, 3, 22, 150, 5, 1, 473, 50, 29, 100, 235, 520, 31, 25, 2, 0, 62, 0, 17, 230, 112]\n",
      "Min partition size: 0\n",
      "Partition sizes: [195, 39, 14, 8, 280, 425, 2, 35, 130, 30, 222, 3, 203, 0, 67, 69, 11, 3, 358, 8, 0, 15, 221, 38, 148, 6, 193, 69, 22, 29, 221, 26, 36, 109, 4, 357, 198, 50, 241, 47, 68, 36, 127, 33, 195, 15, 7, 2, 39, 0, 35, 3, 6, 59, 108, 14, 23, 15, 1, 298]\n",
      "Min partition size: 0\n",
      "Partition sizes: [154, 65, 38, 109, 176, 1, 134, 6, 19, 99, 248, 209, 2, 2, 19, 162, 20, 210, 792, 9, 43, 59, 28, 46, 130, 33, 36, 71, 50, 110, 0, 6, 79, 235, 19, 140, 142, 23, 4, 120, 24, 1, 71, 39, 138, 188, 14, 0, 106, 116, 63, 119, 56, 1, 71, 5, 61, 5, 30, 260]\n",
      "Min partition size: 0\n",
      "Partition sizes: [11, 16, 32, 29, 183, 96, 10, 4, 1, 0, 235, 117, 29, 262, 32, 69, 160, 7, 298, 8, 14, 0, 74, 67, 75, 28, 116, 47, 240, 204, 2, 23, 5, 59, 49, 102, 65, 85, 28, 31, 2, 123, 453, 1, 60, 0, 5, 241, 46, 1, 367, 199, 79, 0, 27, 246, 15, 10, 0, 428]\n",
      "Min partition size: 0\n",
      "Partition sizes: [24, 42, 15, 11, 36, 48, 10, 0, 16, 12, 326, 7, 6, 2, 99, 9, 30, 171, 43, 55, 128, 9, 6, 15, 1, 545, 247, 349, 55, 252, 73, 395, 82, 1, 101, 57, 43, 2, 63, 14, 16, 2, 8, 4, 12, 19, 197, 15, 76, 54, 54, 301, 380, 148, 188, 2, 0, 88, 1, 251]\n",
      "Min partition size: 0\n",
      "Partition sizes: [34, 0, 36, 234, 23, 66, 15, 109, 46, 487, 326, 17, 10, 1, 5, 111, 110, 60, 0, 295, 1, 14, 0, 12, 3, 114, 166, 122, 399, 9, 1, 79, 0, 137, 16, 75, 1, 6, 10, 5, 0, 0, 284, 108, 59, 22, 29, 1, 169, 18, 137, 203, 510, 136, 254, 50, 4, 24, 31, 22]\n",
      "Min partition size: 0\n",
      "Partition sizes: [41, 9, 104, 53, 50, 76, 37, 430, 29, 133, 147, 65, 26, 152, 63, 0, 158, 0, 47, 16, 21, 101, 79, 164, 12, 204, 0, 428, 2, 90, 5, 61, 10, 18, 227, 33, 1, 47, 1, 41, 1, 28, 1, 59, 68, 4, 5, 13, 123, 209, 122, 32, 59, 225, 306, 0, 114, 24, 130, 512]\n",
      "Min partition size: 0\n",
      "Partition sizes: [2, 4, 3, 193, 12, 359, 345, 20, 199, 10, 103, 26, 56, 88, 185, 3, 1, 51, 214, 22, 30, 92, 72, 203, 126, 146, 51, 76, 27, 5, 26, 0, 59, 9, 107, 5, 24, 31, 0, 192, 6, 44, 277, 81, 4, 34, 14, 18, 8, 17, 37, 221, 159, 332, 65, 30, 228, 36, 5, 423]\n",
      "Min partition size: 0\n",
      "Partition sizes: [188, 189, 16, 4, 259, 275, 19, 698, 89, 14, 0, 6, 46, 44, 125, 1, 8, 1, 9, 250, 93, 88, 83, 2, 110, 14, 3, 44, 528, 28, 16, 2, 90, 74, 23, 53, 4, 1, 17, 24, 14, 4, 318, 16, 79, 46, 188, 23, 160, 94, 94, 300, 117, 8, 58, 3, 3, 11, 45, 97]\n",
      "Min partition size: 0\n",
      "Partition sizes: [71, 31, 77, 64, 26, 5, 56, 206, 132, 3, 15, 23, 164, 175, 33, 9, 7, 190, 89, 434, 0, 73, 46, 29, 8, 2, 75, 83, 446, 210, 16, 9, 0, 619, 68, 19, 81, 128, 10, 27, 20, 29, 12, 67, 221, 23, 12, 8, 138, 11, 0, 10, 13, 128, 427, 10, 30, 72, 164, 62]\n",
      "Min partition size: 0\n",
      "Partition sizes: [241, 108, 27, 55, 37, 221, 0, 251, 71, 38, 59, 8, 402, 356, 10, 55, 3, 2, 26, 0, 74, 80, 4, 203, 123, 22, 20, 107, 76, 34, 37, 222, 135, 0, 3, 23, 6, 11, 100, 268, 21, 209, 1, 3, 3, 11, 189, 329, 1, 136, 408, 32, 8, 61, 106, 2, 83, 17, 77, 1]\n",
      "Min partition size: 0\n",
      "Partition sizes: [71, 97, 584, 227, 0, 11, 261, 4, 303, 131, 45, 183, 35, 218, 99, 5, 19, 78, 30, 34, 139, 439, 3, 3, 100, 27, 10, 3, 222, 11, 6, 4, 44, 54, 221, 112, 104, 24, 42, 9, 2, 295, 2, 10, 48, 98, 29, 24, 28, 4, 7, 22, 31, 90, 47, 177, 61, 14, 177, 38]\n",
      "Min partition size: 0\n",
      "Partition sizes: [21, 11, 26, 251, 453, 250, 307, 311, 97, 32, 19, 56, 123, 47, 26, 118, 264, 1, 22, 4, 118, 154, 470, 202, 127, 35, 18, 105, 1, 15, 18, 15, 2, 58, 30, 9, 36, 24, 25, 14, 108, 315, 11, 4, 43, 49, 17, 77, 114, 56, 42, 52, 74, 126, 62, 17, 48, 1, 49, 36]\n",
      "Min partition size: 1\n",
      "Partition 0 class distribution: {1: 14, 0: 7}\n",
      "Partition 1 class distribution: {1: 10, 0: 1}\n",
      "Partition 2 class distribution: {0: 7, 1: 19}\n",
      "Partition 3 class distribution: {1: 176, 0: 75}\n",
      "Partition 4 class distribution: {1: 349, 0: 104}\n",
      "Partition 5 class distribution: {1: 183, 0: 67}\n",
      "Partition 6 class distribution: {1: 215, 0: 92}\n",
      "Partition 7 class distribution: {1: 237, 0: 74}\n",
      "Partition 8 class distribution: {1: 75, 0: 22}\n",
      "Partition 9 class distribution: {1: 23, 0: 9}\n",
      "Partition 10 class distribution: {0: 5, 1: 14}\n",
      "Partition 11 class distribution: {1: 39, 0: 17}\n",
      "Partition 12 class distribution: {1: 83, 0: 40}\n",
      "Partition 13 class distribution: {1: 38, 0: 9}\n",
      "Partition 14 class distribution: {1: 22, 0: 4}\n",
      "Partition 15 class distribution: {1: 82, 0: 36}\n",
      "Partition 16 class distribution: {1: 208, 0: 56}\n",
      "Partition 17 class distribution: {1: 1}\n",
      "Partition 18 class distribution: {1: 18, 0: 4}\n",
      "Partition 19 class distribution: {0: 1, 1: 3}\n",
      "Partition 20 class distribution: {1: 97, 0: 21}\n",
      "Partition 21 class distribution: {1: 110, 0: 44}\n",
      "Partition 22 class distribution: {1: 356, 0: 114}\n",
      "Partition 23 class distribution: {1: 152, 0: 50}\n",
      "Partition 24 class distribution: {1: 90, 0: 37}\n",
      "Partition 25 class distribution: {1: 28, 0: 7}\n",
      "Partition 26 class distribution: {1: 11, 0: 7}\n",
      "Partition 27 class distribution: {1: 79, 0: 26}\n",
      "Partition 28 class distribution: {0: 1}\n",
      "Partition 29 class distribution: {1: 10, 0: 5}\n",
      "Partition 30 class distribution: {1: 16, 0: 2}\n",
      "Partition 31 class distribution: {1: 14, 0: 1}\n",
      "Partition 32 class distribution: {0: 1, 1: 1}\n",
      "Partition 33 class distribution: {1: 44, 0: 14}\n",
      "Partition 34 class distribution: {1: 26, 0: 4}\n",
      "Partition 35 class distribution: {1: 8, 0: 1}\n",
      "Partition 36 class distribution: {0: 6, 1: 30}\n",
      "Partition 37 class distribution: {1: 15, 0: 9}\n",
      "Partition 38 class distribution: {1: 24, 0: 1}\n",
      "Partition 39 class distribution: {1: 9, 0: 5}\n",
      "Partition 40 class distribution: {1: 75, 0: 33}\n",
      "Partition 41 class distribution: {1: 240, 0: 75}\n",
      "Partition 42 class distribution: {1: 9, 0: 2}\n",
      "Partition 43 class distribution: {0: 2, 1: 2}\n",
      "Partition 44 class distribution: {1: 32, 0: 11}\n",
      "Partition 45 class distribution: {1: 39, 0: 10}\n",
      "Partition 46 class distribution: {1: 14, 0: 3}\n",
      "Partition 47 class distribution: {1: 56, 0: 21}\n",
      "Partition 48 class distribution: {1: 76, 0: 38}\n",
      "Partition 49 class distribution: {0: 24, 1: 32}\n",
      "Partition 50 class distribution: {1: 35, 0: 7}\n",
      "Partition 51 class distribution: {1: 33, 0: 19}\n",
      "Partition 52 class distribution: {1: 52, 0: 22}\n",
      "Partition 53 class distribution: {1: 91, 0: 35}\n",
      "Partition 54 class distribution: {1: 49, 0: 13}\n",
      "Partition 55 class distribution: {1: 9, 0: 8}\n",
      "Partition 56 class distribution: {1: 36, 0: 12}\n",
      "Partition 57 class distribution: {0: 1}\n",
      "Partition 58 class distribution: {1: 36, 0: 13}\n",
      "Partition 59 class distribution: {1: 30, 0: 6}\n",
      "Number of train samples: 5216, val samples: 0, test samples: 129\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprepare_quantity_drl_and_noisy_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 89\u001b[0m, in \u001b[0;36mprepare_quantity_drl_and_noisy_data\u001b[0;34m(num_partitions, batch_size, val_ratio, beta, sigma, seed)\u001b[0m\n\u001b[1;32m     87\u001b[0m noisy_image \u001b[38;5;241m=\u001b[39m apply_gaussian_noise(image, partition_std_dev)\n\u001b[1;32m     88\u001b[0m noisy_image \u001b[38;5;241m=\u001b[39m unnormalize_image(noisy_image, mean, std)\n\u001b[0;32m---> 89\u001b[0m noisy_image_pil \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToPILImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m image_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     91\u001b[0m noisy_image_pil\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_dir, image_filename))\n",
      "File \u001b[0;32m/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/torchvision/transforms/transforms.py:226\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/torchvision/transforms/functional.py:334\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnpimg\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/PIL/Image.py:3335\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strides \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtobytes\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3335\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3336\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtostring\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   3337\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prepare_quantity_drl_and_noisy_data(60, 10, 0, 0.5, 0.3, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_quantity_drl_and_noisy_data(60, 10, 0, 0.5, 0.5, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_quantity_skew_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 10, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    all_indices = np.array(trainset.indices)\n",
    "    train_labels = np.array(trainset.dataset.targets)[all_indices]  # Nhãn của toàn bộ dữ liệu trainset\n",
    "    num_labels = len(np.unique(train_labels))  # Số lượng nhãn (classes)\n",
    "\n",
    "    # Đảm bảo mỗi partition có ít nhất một mẫu của mỗi lớp\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    for label in range(num_labels):\n",
    "        label_indices = np.where(train_labels == label)[0]\n",
    "        np.random.shuffle(label_indices)\n",
    "\n",
    "        # Gán một mẫu của lớp `label` cho mỗi partition\n",
    "        if len(label_indices) < num_partitions:\n",
    "            raise ValueError(f\"Không đủ mẫu cho lớp {label} để phân phối cho tất cả partitions.\")\n",
    "        for i in range(num_partitions):\n",
    "            partition_indices[i].append(label_indices[i])\n",
    "        \n",
    "        # Phân phối phần còn lại của lớp `label` theo Dirichlet\n",
    "        remaining_indices = label_indices[num_partitions:]\n",
    "        if len(remaining_indices) > 0:\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "            proportions = (proportions * len(remaining_indices)).astype(int)\n",
    "\n",
    "            # Điều chỉnh proportions để đảm bảo tổng đúng bằng số mẫu còn lại\n",
    "            while proportions.sum() < len(remaining_indices):\n",
    "                proportions[np.argmax(proportions)] += 1\n",
    "            while proportions.sum() > len(remaining_indices):\n",
    "                proportions[np.argmax(proportions)] -= 1\n",
    "\n",
    "            splits = np.split(remaining_indices, np.cumsum(proportions)[:-1])\n",
    "            for i, split in enumerate(splits):\n",
    "                partition_indices[i].extend(split.tolist())\n",
    "\n",
    "    # Kiểm tra kích thước tối thiểu của các partition\n",
    "    min_size = min([len(part) for part in partition_indices])\n",
    "    if min_size < 1:\n",
    "        raise ValueError(\"Một partition không đủ mẫu sau khi phân phối.\")\n",
    "\n",
    "    # Tạo các tập train và val từ partition_indices\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    # Phân chia valset thành các partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Tạo DataLoaders\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=6) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "    # Phân tích phân bố lớp\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "\n",
    "    # Vẽ biểu đồ phân bố lớp\n",
    "    partitions = range(num_partitions)\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bottom = np.zeros(num_partitions)\n",
    "    colors = plt.cm.tab10.colors\n",
    "\n",
    "    for cls in range(num_labels):\n",
    "        counts = [class_distributions[i].get(cls, 0) for i in partitions]\n",
    "        plt.bar(partitions, counts, bar_width, bottom=bottom, label=f'Class {cls}', color=colors[cls % len(colors)])\n",
    "        bottom += counts\n",
    "\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition_quantity_skew.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_quantity_skew_dirichlet_with_noise(num_partitions: int, batch_size: int, val_ratio: float = 0.1, \n",
    "                                               beta: float = 10, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Phân phối dữ liệu theo quantity skew Dirichlet, thêm nhiễu Gaussian vào từng partition.\n",
    "    \"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    all_indices = np.array(trainset.indices)\n",
    "    train_labels = np.array(trainset.dataset.targets)[all_indices]\n",
    "    num_labels = len(np.unique(train_labels))\n",
    "\n",
    "    # Đảm bảo mỗi partition có ít nhất một mẫu của mỗi lớp\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    for label in range(num_labels):\n",
    "        label_indices = np.where(train_labels == label)[0]\n",
    "        np.random.shuffle(label_indices)\n",
    "\n",
    "        # Gán ít nhất 1 mẫu của mỗi lớp cho từng partition\n",
    "        if len(label_indices) < num_partitions:\n",
    "            raise ValueError(f\"Không đủ mẫu cho lớp {label} để phân phối cho tất cả partitions.\")\n",
    "        for i in range(num_partitions):\n",
    "            partition_indices[i].append(label_indices[i])\n",
    "\n",
    "        # Phân phối phần còn lại của lớp theo Dirichlet\n",
    "        remaining_indices = label_indices[num_partitions:]\n",
    "        if len(remaining_indices) > 0:\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "            proportions = (proportions * len(remaining_indices)).astype(int)\n",
    "\n",
    "            # Điều chỉnh proportions để tổng khớp\n",
    "            while proportions.sum() < len(remaining_indices):\n",
    "                proportions[np.argmax(proportions)] += 1\n",
    "            while proportions.sum() > len(remaining_indices):\n",
    "                proportions[np.argmax(proportions)] -= 1\n",
    "\n",
    "            splits = np.split(remaining_indices, np.cumsum(proportions)[:-1])\n",
    "            for i, split in enumerate(splits):\n",
    "                partition_indices[i].extend(split.tolist())\n",
    "\n",
    "    # Kiểm tra tính hợp lệ\n",
    "    min_size = min([len(part) for part in partition_indices])\n",
    "    if min_size < 1:\n",
    "        raise ValueError(\"Một partition không đủ mẫu sau khi phân phối.\")\n",
    "\n",
    "    # Thêm nhiễu Gaussian và lưu dữ liệu\n",
    "    noise_dir = f\"{BASE_FOLDER_NOISE}/quantity_skew_dirichlet_with_noise_beta_{beta}_sigma_{sigma}\"\n",
    "    os.makedirs(noise_dir, exist_ok=True)\n",
    "\n",
    "    trainsets = []\n",
    "    for i, indices in enumerate(partition_indices):\n",
    "        partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "        os.makedirs(partition_dir, exist_ok=True)\n",
    "\n",
    "        partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "        noisy_data = []\n",
    "\n",
    "        for idx in indices:\n",
    "            image, label = trainset.dataset[idx]\n",
    "            noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "            noisy_data.append((noisy_image, label))\n",
    "\n",
    "            # Lưu ảnh nhiễu Gaussian\n",
    "            class_dir = os.path.join(partition_dir, f'class_{label}')\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            noisy_image = unnormalize_image(noisy_image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "            image_filename = f'image_{idx}.png'\n",
    "            noisy_image_pil.save(os.path.join(class_dir, image_filename))\n",
    "\n",
    "        trainsets.append(noisy_data)\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=6) for ts in trainsets]\n",
    "\n",
    "    # Phân chia valset thành các partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "    # Phân tích và hiển thị phân bố lớp\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "\n",
    "    # Vẽ biểu đồ phân bố\n",
    "    partitions = range(num_partitions)\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bottom = np.zeros(num_partitions)\n",
    "    colors = plt.cm.tab10.colors\n",
    "\n",
    "    for cls in range(num_labels):\n",
    "        counts = [class_distributions[i].get(cls, 0) for i in partitions]\n",
    "        plt.bar(partitions, counts, bar_width, bottom=bottom, label=f'Class {cls}', color=colors[cls % len(colors)])\n",
    "        bottom += counts\n",
    "\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition_quantity_skew_with_noise.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0 class distribution: {1: 49, 0: 17}\n",
      "Partition 1 class distribution: {1: 55, 0: 18}\n",
      "Partition 2 class distribution: {1: 18, 0: 8}\n",
      "Partition 3 class distribution: {1: 18, 0: 8}\n",
      "Number of train samples: 191, val samples: 0, test samples: 129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<torch.utils.data.dataloader.DataLoader at 0x7dde46eaf400>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde40aca2c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde40ac9480>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde40ac9210>],\n",
       " [<torch.utils.data.dataloader.DataLoader at 0x7dde40acba90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde40ac9240>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47023c10>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7dde47023d00>],\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7dde46ead570>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_quantity_skew_dirichlet_with_noise(4, 10, 0, 0.5, 0.5, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_quantity_skew_dirichlet_with_noise(num_partitions: int, batch_size: int, val_ratio: float = 0.1, \n",
    "                                               beta: float = 10, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Phân phối dữ liệu theo quantity skew Dirichlet, thêm nhiễu Gaussian vào từng partition.\n",
    "    Nếu dữ liệu đã được lưu, chỉ cần tải lại mà không tạo mới.\n",
    "    \"\"\"\n",
    "    noise_dir = f\"{BASE_FOLDER_NOISE}/quantity_skew_dirichlet_with_noise_beta_{beta}_sigma_{sigma}\"\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    # Nếu thư mục tồn tại, tải dữ liệu\n",
    "    if os.path.exists(noise_dir):\n",
    "        print(f\"Loading partitioned and noisy dataset from {noise_dir}...\")\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "                            for i in range(num_partitions)]\n",
    "        num_labels = len(train_partitions[0].classes)\n",
    "\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "                       for part in train_partitions]\n",
    "\n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "        print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "        # return trainloaders, valloaders, testloader\n",
    "\n",
    "    else:\n",
    "        # Nếu không, tạo mới dữ liệu và thêm nhiễu\n",
    "        print(f\"Creating partitioned and noisy dataset and saving to {noise_dir}...\")\n",
    "        os.makedirs(noise_dir, exist_ok=True)\n",
    "\n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "        all_indices = np.array(trainset.indices)\n",
    "        train_labels = np.array(trainset.dataset.targets)[all_indices]\n",
    "        num_labels = len(np.unique(train_labels))\n",
    "\n",
    "        # Đảm bảo mỗi partition có ít nhất một mẫu của mỗi lớp\n",
    "        partition_indices = [[] for _ in range(num_partitions)]\n",
    "        for label in range(num_labels):\n",
    "            label_indices = np.where(train_labels == label)[0]\n",
    "            np.random.shuffle(label_indices)\n",
    "\n",
    "            # Gán ít nhất 1 mẫu của mỗi lớp cho từng partition\n",
    "            if len(label_indices) < num_partitions:\n",
    "                raise ValueError(f\"Không đủ mẫu cho lớp {label} để phân phối cho tất cả partitions.\")\n",
    "            for i in range(num_partitions):\n",
    "                partition_indices[i].append(label_indices[i])\n",
    "\n",
    "            # Phân phối phần còn lại của lớp theo Dirichlet\n",
    "            remaining_indices = label_indices[num_partitions:]\n",
    "            if len(remaining_indices) > 0:\n",
    "                proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "                proportions = (proportions * len(remaining_indices)).astype(int)\n",
    "\n",
    "                # Điều chỉnh proportions để tổng khớp\n",
    "                while proportions.sum() < len(remaining_indices):\n",
    "                    proportions[np.argmax(proportions)] += 1\n",
    "                while proportions.sum() > len(remaining_indices):\n",
    "                    proportions[np.argmax(proportions)] -= 1\n",
    "\n",
    "                splits = np.split(remaining_indices, np.cumsum(proportions)[:-1])\n",
    "                for i, split in enumerate(splits):\n",
    "                    partition_indices[i].extend(split.tolist())\n",
    "\n",
    "        # Thêm nhiễu Gaussian và lưu dữ liệu\n",
    "        trainsets = []\n",
    "        for i, indices in enumerate(partition_indices):\n",
    "            partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "            os.makedirs(partition_dir, exist_ok=True)\n",
    "\n",
    "            partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "            noisy_data = []\n",
    "\n",
    "            for idx in indices:\n",
    "                image, label = trainset.dataset[idx]\n",
    "                noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "                noisy_data.append((noisy_image, label))\n",
    "\n",
    "                # Lưu ảnh nhiễu Gaussian\n",
    "                class_dir = os.path.join(partition_dir, f'class_{label}')\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "                noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "                image_filename = f'image_{idx}.png'\n",
    "                noisy_image_pil.save(os.path.join(class_dir, image_filename))\n",
    "\n",
    "            trainsets.append(noisy_data)\n",
    "\n",
    "        trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=6) for ts in trainsets]\n",
    "\n",
    "        # Phân chia valset thành các partitions\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "        print(f\"Dữ liệu đã được phân chia và lưu tại {noise_dir}\")\n",
    "    \n",
    "    # Phân tích phân bố lớp\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    # Vẽ biểu đồ phân bố lớp\n",
    "    partitions = range(num_partitions)\n",
    "    class_counts_list = []\n",
    "    for i in partitions:\n",
    "        counts = {cls: class_distributions[i].get(cls, 0) for cls in range(num_labels)}\n",
    "        class_counts_list.append(counts)\n",
    "    \n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bottom = np.zeros(num_partitions)\n",
    "    colors = plt.cm.tab10.colors  # Sử dụng bảng màu có sẵn\n",
    "    \n",
    "    for cls in range(num_labels):\n",
    "        counts = [class_counts_list[i].get(cls, 0) for i in partitions]\n",
    "        plt.bar(partitions, counts, bar_width, bottom=bottom, label=f'Class {cls}', color=colors[cls % len(colors)])\n",
    "        bottom += counts\n",
    "    \n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition_combined.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f'Number of train samples: {sum(len(loader.dataset) for loader in trainloaders)}, '\n",
    "          f'val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    \n",
    "    return trainloaders, valloaders, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating partitioned and noisy dataset and saving to /media/namvq/Data/code_chinh_sua/fedavg/quantity_skew_dirichlet_with_noise_beta_0.5_sigma_0.1...\n",
      "Dữ liệu đã được phân chia và lưu tại /media/namvq/Data/code_chinh_sua/fedavg/quantity_skew_dirichlet_with_noise_beta_0.5_sigma_0.1\n",
      "Partition 0 class distribution: {0: 9, 1: 17}\n",
      "Partition 1 class distribution: {0: 38, 1: 128}\n",
      "Partition 2 class distribution: {1: 228, 0: 64}\n",
      "Partition 3 class distribution: {0: 19, 1: 33}\n",
      "Partition 4 class distribution: {1: 23, 0: 8}\n",
      "Partition 5 class distribution: {1: 137, 0: 52}\n",
      "Partition 6 class distribution: {1: 3, 0: 1}\n",
      "Partition 7 class distribution: {1: 69, 0: 24}\n",
      "Partition 8 class distribution: {1: 173, 0: 48}\n",
      "Partition 9 class distribution: {1: 32, 0: 18}\n",
      "Partition 10 class distribution: {0: 22, 1: 84}\n",
      "Partition 11 class distribution: {0: 3, 1: 9}\n",
      "Partition 12 class distribution: {1: 99, 0: 26}\n",
      "Partition 13 class distribution: {1: 27, 0: 14}\n",
      "Partition 14 class distribution: {0: 27, 1: 94}\n",
      "Partition 15 class distribution: {1: 49, 0: 12}\n",
      "Partition 16 class distribution: {0: 22, 1: 75}\n",
      "Partition 17 class distribution: {1: 124, 0: 46}\n",
      "Partition 18 class distribution: {1: 57, 0: 14}\n",
      "Partition 19 class distribution: {1: 92, 0: 28}\n",
      "Partition 20 class distribution: {1: 72, 0: 23}\n",
      "Partition 21 class distribution: {1: 10, 0: 3}\n",
      "Partition 22 class distribution: {1: 13, 0: 4}\n",
      "Partition 23 class distribution: {1: 31, 0: 7}\n",
      "Partition 24 class distribution: {1: 30, 0: 11}\n",
      "Partition 25 class distribution: {1: 8}\n",
      "Partition 26 class distribution: {1: 9, 0: 2}\n",
      "Partition 27 class distribution: {0: 1, 1: 3}\n",
      "Partition 28 class distribution: {1: 20, 0: 11}\n",
      "Partition 29 class distribution: {1: 19, 0: 3}\n",
      "Partition 30 class distribution: {0: 15, 1: 38}\n",
      "Partition 31 class distribution: {1: 116, 0: 42}\n",
      "Partition 32 class distribution: {1: 59, 0: 25}\n",
      "Partition 33 class distribution: {1: 85, 0: 40}\n",
      "Partition 34 class distribution: {1: 9, 0: 1}\n",
      "Partition 35 class distribution: {1: 141, 0: 55}\n",
      "Partition 36 class distribution: {0: 3, 1: 11}\n",
      "Partition 37 class distribution: {1: 96, 0: 40}\n",
      "Partition 38 class distribution: {1: 92, 0: 30}\n",
      "Partition 39 class distribution: {1: 49, 0: 22}\n",
      "Partition 40 class distribution: {1: 10, 0: 3}\n",
      "Partition 41 class distribution: {1: 114, 0: 41}\n",
      "Partition 42 class distribution: {0: 8, 1: 14}\n",
      "Partition 43 class distribution: {1: 21, 0: 11}\n",
      "Partition 44 class distribution: {1: 2, 0: 1}\n",
      "Partition 45 class distribution: {1: 67, 0: 25}\n",
      "Partition 46 class distribution: {1: 179, 0: 62}\n",
      "Partition 47 class distribution: {1: 28, 0: 14}\n",
      "Partition 48 class distribution: {1: 46, 0: 13}\n",
      "Partition 49 class distribution: {1: 62, 0: 28}\n",
      "Partition 50 class distribution: {1: 14, 0: 11}\n",
      "Partition 51 class distribution: {0: 7, 1: 12}\n",
      "Partition 52 class distribution: {0: 105, 1: 260}\n",
      "Partition 53 class distribution: {1: 43, 0: 6}\n",
      "Partition 54 class distribution: {1: 11}\n",
      "Partition 55 class distribution: {0: 13, 1: 38}\n",
      "Partition 56 class distribution: {0: 123, 1: 417}\n",
      "Partition 57 class distribution: {1: 12, 0: 5}\n",
      "Partition 58 class distribution: {1: 19, 0: 9}\n",
      "Partition 59 class distribution: {1: 42, 0: 23}\n",
      "Number of train samples: 5216, val samples: 0, test samples: 624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<torch.utils.data.dataloader.DataLoader at 0x7c94b4f05d20>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4147f10>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f14e80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f11330>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f10ac0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f11630>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4db70>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4cb50>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c910>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4ca90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c7c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c580>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c4c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c5b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c430>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4f0a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4efe0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c400>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4f1f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c1c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c160>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c0a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4f7c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4fe20>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4c040>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4cca0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4ce20>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4cd30>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32650>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32620>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32530>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32440>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32e30>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32fb0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33070>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f328c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32830>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33250>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f332e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f334c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33400>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33760>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33610>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f335b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33880>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f339a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33b80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33ac0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33c70>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33d60>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33d30>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f33ee0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f30340>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32d40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32c80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32b90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32b00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f30100>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f329e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f30070>],\n",
       " [<torch.utils.data.dataloader.DataLoader at 0x7c94b4f30220>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f305e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f31b70>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f31ba0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f31c60>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f31e70>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f31d80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32110>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32020>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f32170>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f322c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b1c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411bf40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411bd00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411bd60>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411bd30>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411bc40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b490>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b4c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411bb80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411bbb0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b850>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b5e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b790>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b5b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b3a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b190>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b1f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411aef0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411af50>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411b0a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411abc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411ac80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411add0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a8f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411aa10>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a920>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a770>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a830>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a620>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a5c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a440>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a2f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a380>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4119f90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a170>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c411a1a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4119e40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4119e70>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4119c90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4119bd0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4119de0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c41199c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4119ab0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4119840>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c41198a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4119540>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c41195d0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c41191b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4119150>],\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7c94b4f07c70>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_quantity_skew_dirichlet_with_noise(60, 10, 0, 0.5, 0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating partitioned and noisy dataset and saving to /media/namvq/Data/code_chinh_sua/fedavg/quantity_skew_dirichlet_with_noise_beta_0.5_sigma_0.3...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprepare_quantity_skew_dirichlet_with_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 94\u001b[0m, in \u001b[0;36mprepare_quantity_skew_dirichlet_with_noise\u001b[0;34m(num_partitions, batch_size, val_ratio, beta, sigma, seed)\u001b[0m\n\u001b[1;32m     91\u001b[0m noisy_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[0;32m---> 94\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m \u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     95\u001b[0m     noisy_image \u001b[38;5;241m=\u001b[39m apply_gaussian_noise(image, partition_std_dev)\n\u001b[1;32m     96\u001b[0m     noisy_data\u001b[38;5;241m.\u001b[39mappend((noisy_image, label))\n",
      "File \u001b[0;32m/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/torchvision/datasets/folder.py:230\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 230\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/torchvision/datasets/folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/torchvision/datasets/folder.py:249\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    248\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/PIL/Image.py:1152\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     dither \u001b[38;5;241m=\u001b[39m Dither\u001b[38;5;241m.\u001b[39mFLOYDSTEINBERG\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdither\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prepare_quantity_skew_dirichlet_with_noise(60, 10, 0, 0.5, 0.3, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating partitioned and noisy dataset and saving to /media/namvq/Data/code_chinh_sua/fedavg/quantity_skew_dirichlet_with_noise_beta_0.5_sigma_0.5...\n",
      "Dữ liệu đã được phân chia và lưu tại /media/namvq/Data/code_chinh_sua/fedavg/quantity_skew_dirichlet_with_noise_beta_0.5_sigma_0.5\n",
      "Partition 0 class distribution: {0: 36, 1: 120}\n",
      "Partition 1 class distribution: {0: 35, 1: 167}\n",
      "Partition 2 class distribution: {1: 47, 0: 13}\n",
      "Partition 3 class distribution: {1: 80, 0: 31}\n",
      "Partition 4 class distribution: {1: 4, 0: 1}\n",
      "Partition 5 class distribution: {0: 1, 1: 5}\n",
      "Partition 6 class distribution: {1: 106, 0: 34}\n",
      "Partition 7 class distribution: {0: 4, 1: 2}\n",
      "Partition 8 class distribution: {1: 197, 0: 63}\n",
      "Partition 9 class distribution: {1: 30, 0: 6}\n",
      "Partition 10 class distribution: {0: 10, 1: 14}\n",
      "Partition 11 class distribution: {1: 3, 0: 1}\n",
      "Partition 12 class distribution: {1: 76, 0: 29}\n",
      "Partition 13 class distribution: {1: 59, 0: 21}\n",
      "Partition 14 class distribution: {1: 110, 0: 41}\n",
      "Partition 15 class distribution: {1: 76, 0: 26}\n",
      "Partition 16 class distribution: {1: 87, 0: 37}\n",
      "Partition 17 class distribution: {1: 12, 0: 3}\n",
      "Partition 18 class distribution: {1: 1, 0: 1}\n",
      "Partition 19 class distribution: {1: 62, 0: 20}\n",
      "Partition 20 class distribution: {1: 54, 0: 16}\n",
      "Partition 21 class distribution: {1: 11, 0: 3}\n",
      "Partition 22 class distribution: {0: 32, 1: 93}\n",
      "Partition 23 class distribution: {0: 32, 1: 110}\n",
      "Partition 24 class distribution: {0: 19, 1: 62}\n",
      "Partition 25 class distribution: {1: 5, 0: 4}\n",
      "Partition 26 class distribution: {0: 79, 1: 230}\n",
      "Partition 27 class distribution: {1: 32, 0: 11}\n",
      "Partition 28 class distribution: {1: 57, 0: 15}\n",
      "Partition 29 class distribution: {0: 66, 1: 147}\n",
      "Partition 30 class distribution: {1: 124, 0: 39}\n",
      "Partition 31 class distribution: {1: 140, 0: 63}\n",
      "Partition 32 class distribution: {1: 79, 0: 29}\n",
      "Partition 33 class distribution: {1: 262, 0: 95}\n",
      "Partition 34 class distribution: {0: 17, 1: 29}\n",
      "Partition 35 class distribution: {0: 4, 1: 11}\n",
      "Partition 36 class distribution: {0: 32, 1: 76}\n",
      "Partition 37 class distribution: {1: 47, 0: 22}\n",
      "Partition 38 class distribution: {0: 4, 1: 23}\n",
      "Partition 39 class distribution: {1: 20, 0: 7}\n",
      "Partition 40 class distribution: {1: 13, 0: 2}\n",
      "Partition 41 class distribution: {1: 14, 0: 3}\n",
      "Partition 42 class distribution: {1: 21, 0: 4}\n",
      "Partition 43 class distribution: {0: 4, 1: 8}\n",
      "Partition 44 class distribution: {1: 28, 0: 4}\n",
      "Partition 45 class distribution: {1: 183, 0: 67}\n",
      "Partition 46 class distribution: {0: 18, 1: 56}\n",
      "Partition 47 class distribution: {1: 42, 0: 18}\n",
      "Partition 48 class distribution: {0: 23, 1: 87}\n",
      "Partition 49 class distribution: {1: 65, 0: 19}\n",
      "Partition 50 class distribution: {1: 36, 0: 15}\n",
      "Partition 51 class distribution: {0: 21, 1: 44}\n",
      "Partition 52 class distribution: {1: 89, 0: 35}\n",
      "Partition 53 class distribution: {1: 2, 0: 2}\n",
      "Partition 54 class distribution: {0: 33, 1: 142}\n",
      "Partition 55 class distribution: {1: 9, 0: 2}\n",
      "Partition 56 class distribution: {0: 12, 1: 39}\n",
      "Partition 57 class distribution: {0: 14, 1: 41}\n",
      "Partition 58 class distribution: {1: 62, 0: 31}\n",
      "Partition 59 class distribution: {1: 24, 0: 12}\n",
      "Number of train samples: 5216, val samples: 0, test samples: 624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<torch.utils.data.dataloader.DataLoader at 0x7fae21878970>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21d83700>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fad8c088730>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae218a9ed0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21aaaad0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fad8c06ed40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fad8c06f520>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009aa40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009b490>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009be50>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009abc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30099b40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30098e20>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30098520>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009a5c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009bee0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009ab60>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009b730>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009bac0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009add0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009a9e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009ac80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae300985b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009bca0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009b250>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009bdf0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009bcd0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30099cf0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30099090>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30098dc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30099c00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30099de0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30098d30>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae3009b310>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30098df0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae300989a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30099150>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae30099a20>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c288b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c2a770>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c2ae60>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c297b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a84340>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a844c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a84430>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a844f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a84880>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a847f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a863b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a85d80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a85fc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a85510>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a87130>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a85db0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a87850>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a87490>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a86e90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a878b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a85330>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a876a0>],\n",
       " [<torch.utils.data.dataloader.DataLoader at 0x7fae21a85870>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a85f90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21dc74f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21dc7520>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21dc4dc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c11120>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c109d0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c116f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21de2fb0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21de26b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21de3370>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21bf6b30>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21bf5ae0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21bf7f70>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21bf6a40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c40700>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c42f50>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c419f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c436d0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21c435b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58af0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58e50>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a59000>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a59a50>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a59c00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a59120>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5a740>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5a230>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58820>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5be20>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58c10>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58c40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a593f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58550>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a588e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5b880>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5baf0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5bc10>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5bd30>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5bd60>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5bf40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5ab00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5b1f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5b6a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5b730>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5b790>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58580>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a585b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58460>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58310>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5a560>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a581f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58a60>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58cd0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a582b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58e20>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58910>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58ee0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a5a110>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7fae21a58be0>],\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fad97f52980>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_quantity_skew_dirichlet_with_noise(60, 10, 0, 0.5, 0.5, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = get_custom_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_quantity_skew_dirichlet_with_noise(num_partitions: int, batch_size: int, val_ratio: float = 0.1, \n",
    "                                               beta: float = 10, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Phân phối dữ liệu theo quantity skew Dirichlet, thêm nhiễu Gaussian vào từng partition.\n",
    "    Nếu dữ liệu đã được lưu, chỉ cần tải lại mà không tạo mới.\n",
    "    \"\"\"\n",
    "    noise_dir = f\"{BASE_FOLDER_NOISE}/quantity_skew_dirichlet_with_noise_beta_{beta}_sigma_{sigma}\"\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    # Nếu thư mục tồn tại, tải dữ liệu\n",
    "    if os.path.exists(noise_dir):\n",
    "        print(f\"Loading partitioned and noisy dataset from {noise_dir}...\")\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "                            for i in range(num_partitions)]\n",
    "        num_labels = len(train_partitions[0].classes)\n",
    "\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "                       for part in train_partitions]\n",
    "\n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "        print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "        # return trainloaders, valloaders, testloader\n",
    "\n",
    "    else:\n",
    "        print(f\"Creating partitioned and noisy dataset and saving to {noise_dir}...\")\n",
    "        os.makedirs(noise_dir, exist_ok=True)\n",
    "\n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "        all_indices = np.array(trainset.indices)\n",
    "        train_labels = np.array(trainset.dataset.targets)[all_indices]\n",
    "        num_labels = len(np.unique(train_labels))\n",
    "        \n",
    "        # Verify we have enough samples per class\n",
    "        for label in range(num_labels):\n",
    "            label_count = np.sum(train_labels == label)\n",
    "            if label_count < num_partitions:\n",
    "                raise ValueError(f\"Class {label} has only {label_count} samples, need at least {num_partitions}\")\n",
    "\n",
    "        # Initialize partitions with minimum samples\n",
    "        partition_indices = [[] for _ in range(num_partitions)]\n",
    "        \n",
    "        # First pass: ensure minimum samples\n",
    "        for label in range(num_labels):\n",
    "            label_indices = np.where(train_labels == label)[0]\n",
    "            np.random.shuffle(label_indices)\n",
    "            \n",
    "            # Assign one sample per partition\n",
    "            for i in range(num_partitions):\n",
    "                partition_indices[i].append(label_indices[i])\n",
    "            \n",
    "            # Distribute remaining samples using Dirichlet\n",
    "            remaining_indices = label_indices[num_partitions:]\n",
    "            if len(remaining_indices) > 0:\n",
    "                proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "                # Convert to actual counts\n",
    "                counts = (proportions * len(remaining_indices)).astype(int)\n",
    "                \n",
    "                # Adjust for rounding errors\n",
    "                remainder = len(remaining_indices) - counts.sum()\n",
    "                if remainder > 0:\n",
    "                    counts[np.argmax(proportions)] += remainder\n",
    "                \n",
    "                # Split and distribute remaining indices\n",
    "                start_idx = 0\n",
    "                for i in range(num_partitions):\n",
    "                    end_idx = start_idx + counts[i]\n",
    "                    if end_idx > start_idx:\n",
    "                        partition_indices[i].extend(remaining_indices[start_idx:end_idx])\n",
    "                    start_idx = end_idx\n",
    "\n",
    "        # Verify distribution\n",
    "        for i, indices in enumerate(partition_indices):\n",
    "            labels = train_labels[indices]\n",
    "            unique_labels = np.unique(labels)\n",
    "            if len(unique_labels) != num_labels:\n",
    "                raise ValueError(f\"Partition {i} is missing classes: {set(range(num_labels)) - set(unique_labels)}\")\n",
    "\n",
    "        # Thêm nhiễu Gaussian và lưu dữ liệu\n",
    "        trainsets = []\n",
    "        for i, indices in enumerate(partition_indices):\n",
    "            partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "            os.makedirs(partition_dir, exist_ok=True)\n",
    "\n",
    "            partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "            noisy_data = []\n",
    "\n",
    "            for idx in indices:\n",
    "                image, label = trainset.dataset[idx]\n",
    "                noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "                noisy_data.append((noisy_image, label))\n",
    "\n",
    "                # Lưu ảnh nhiễu Gaussian\n",
    "                class_dir = os.path.join(partition_dir, f'class_{label}')\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "                noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "                image_filename = f'image_{idx}.png'\n",
    "                noisy_image_pil.save(os.path.join(class_dir, image_filename))\n",
    "\n",
    "            trainsets.append(noisy_data)\n",
    "\n",
    "        trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=6) for ts in trainsets]\n",
    "\n",
    "        # Phân chia valset thành các partitions\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "        print(f\"Dữ liệu đã được phân chia và lưu tại {noise_dir}\")\n",
    "    \n",
    "    # Phân tích phân bố lớp\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    # Vẽ biểu đồ phân bố lớp\n",
    "    partitions = range(num_partitions)\n",
    "    class_counts_list = []\n",
    "    for i in partitions:\n",
    "        counts = {cls: class_distributions[i].get(cls, 0) for cls in range(num_labels)}\n",
    "        class_counts_list.append(counts)\n",
    "    \n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bottom = np.zeros(num_partitions)\n",
    "    colors = plt.cm.tab10.colors  # Sử dụng bảng màu có sẵn\n",
    "    \n",
    "    for cls in range(num_labels):\n",
    "        counts = [class_counts_list[i].get(cls, 0) for i in partitions]\n",
    "        plt.bar(partitions, counts, bar_width, bottom=bottom, label=f'Class {cls}', color=colors[cls % len(colors)])\n",
    "        bottom += counts\n",
    "    \n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition_combined.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f'Number of train samples: {sum(len(loader.dataset) for loader in trainloaders)}, '\n",
    "          f'val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    \n",
    "    return trainloaders, valloaders, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating partitioned and noisy dataset and saving to /media/namvq/Data/code_chinh_sua/fedavg/quantity_skew_dirichlet_with_noise_beta_0.5_sigma_0.5...\n",
      "Dữ liệu đã được phân chia và lưu tại /media/namvq/Data/code_chinh_sua/fedavg/quantity_skew_dirichlet_with_noise_beta_0.5_sigma_0.5\n",
      "Partition 0 class distribution: {1: 15, 0: 5}\n",
      "Partition 1 class distribution: {1: 287, 0: 77}\n",
      "Partition 2 class distribution: {0: 42, 1: 117}\n",
      "Partition 3 class distribution: {1: 4}\n",
      "Partition 4 class distribution: {1: 22, 0: 14}\n",
      "Partition 5 class distribution: {0: 11, 1: 12}\n",
      "Partition 6 class distribution: {1: 69, 0: 24}\n",
      "Partition 7 class distribution: {1: 17, 0: 6}\n",
      "Partition 8 class distribution: {1: 12, 0: 4}\n",
      "Partition 9 class distribution: {1: 48, 0: 17}\n",
      "Partition 10 class distribution: {1: 10, 0: 10}\n",
      "Partition 11 class distribution: {0: 4, 1: 5}\n",
      "Partition 12 class distribution: {0: 38, 1: 86}\n",
      "Partition 13 class distribution: {1: 34, 0: 14}\n",
      "Partition 14 class distribution: {0: 61, 1: 198}\n",
      "Partition 15 class distribution: {1: 148, 0: 46}\n",
      "Partition 16 class distribution: {0: 7, 1: 15}\n",
      "Partition 17 class distribution: {0: 1, 1: 1}\n",
      "Partition 18 class distribution: {1: 91, 0: 30}\n",
      "Partition 19 class distribution: {1: 109, 0: 34}\n",
      "Partition 20 class distribution: {0: 8, 1: 15}\n",
      "Partition 21 class distribution: {1: 16, 0: 5}\n",
      "Partition 22 class distribution: {0: 62, 1: 149}\n",
      "Partition 23 class distribution: {1: 42, 0: 13}\n",
      "Partition 24 class distribution: {1: 45, 0: 18}\n",
      "Partition 25 class distribution: {1: 138, 0: 56}\n",
      "Partition 26 class distribution: {1: 99, 0: 27}\n",
      "Partition 27 class distribution: {0: 46, 1: 108}\n",
      "Partition 28 class distribution: {1: 120, 0: 51}\n",
      "Partition 29 class distribution: {1: 42, 0: 18}\n",
      "Partition 30 class distribution: {1: 29, 0: 13}\n",
      "Partition 31 class distribution: {1: 27, 0: 8}\n",
      "Partition 32 class distribution: {1: 24, 0: 9}\n",
      "Partition 33 class distribution: {1: 84, 0: 31}\n",
      "Partition 34 class distribution: {1: 42, 0: 14}\n",
      "Partition 35 class distribution: {1: 58, 0: 21}\n",
      "Partition 36 class distribution: {0: 1, 1: 6}\n",
      "Partition 37 class distribution: {1: 52, 0: 21}\n",
      "Partition 38 class distribution: {1: 54, 0: 23}\n",
      "Partition 39 class distribution: {1: 287, 0: 77}\n",
      "Partition 40 class distribution: {1: 21, 0: 7}\n",
      "Partition 41 class distribution: {1: 9, 0: 3}\n",
      "Partition 42 class distribution: {1: 21, 0: 4}\n",
      "Partition 43 class distribution: {1: 2, 0: 2}\n",
      "Partition 44 class distribution: {0: 4, 1: 11}\n",
      "Partition 45 class distribution: {0: 22, 1: 77}\n",
      "Partition 46 class distribution: {1: 7, 0: 3}\n",
      "Partition 47 class distribution: {1: 4, 0: 1}\n",
      "Partition 48 class distribution: {1: 18, 0: 2}\n",
      "Partition 49 class distribution: {1: 192, 0: 61}\n",
      "Partition 50 class distribution: {0: 5, 1: 7}\n",
      "Partition 51 class distribution: {0: 62, 1: 182}\n",
      "Partition 52 class distribution: {1: 72, 0: 21}\n",
      "Partition 53 class distribution: {1: 193, 0: 69}\n",
      "Partition 54 class distribution: {1: 43, 0: 6}\n",
      "Partition 55 class distribution: {1: 94, 0: 30}\n",
      "Partition 56 class distribution: {0: 8, 1: 23}\n",
      "Partition 57 class distribution: {0: 21, 1: 53}\n",
      "Partition 58 class distribution: {0: 25, 1: 62}\n",
      "Partition 59 class distribution: {0: 18, 1: 47}\n",
      "Number of train samples: 5216, val samples: 0, test samples: 624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<torch.utils.data.dataloader.DataLoader at 0x7c94c4146a70>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb038b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c9410910610>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c9410910340>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94109102b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94109111b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c9410911ab0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c941090faf0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c941090f730>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c941090c760>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c941090d930>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c941090c100>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c941090ee90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c941090d8d0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c941090c4f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb2ffa0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb2ee60>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb2ea70>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb2f4c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb2f3d0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb2f250>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb2f1c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb2f670>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb2f730>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb2d4e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb26a40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb24880>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb26b90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb25c90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb25d80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb263b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb26260>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb25ff0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb25c00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb25f00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb26680>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb26830>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb26560>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb27670>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb274c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb273d0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb24a30>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb24820>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb24b80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb24970>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb24af0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb24b50>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb24190>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb25870>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb25600>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb25840>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb25930>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb27a00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb259c0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb27a90>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb27e50>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb278b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb279a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb27850>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb26140>],\n",
       " [<torch.utils.data.dataloader.DataLoader at 0x7c940cb275b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940cb26c50>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4d4e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4d0f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4cbb0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f4d690>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ec83910>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ec83760>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ec83850>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ec836d0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ec832b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ec834f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ec833d0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ec82ad0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ec826b0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ee58d00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409bfd0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409b4f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409afb0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409b0a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4099210>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409bbe0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4099360>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409be80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409acb0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409a020>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409bd60>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4098ac0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c40987f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4099ea0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4098fd0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409b2e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4098df0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409ae30>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4098b80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c4099e40>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c40982e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94c409b5e0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942ecbf490>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f15cf0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f14820>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f164a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f15d20>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f15330>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f14af0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f159f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f14dc0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942f87b280>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942f8fab00>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942f8f98a0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942f8f9ba0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942f8fbac0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c942f8fbd60>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f06f20>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f071f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c94b4f07430>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940c77d3f0>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940c77c070>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940c77c160>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7c940c77c1f0>],\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7c942fb51870>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_quantity_skew_dirichlet_with_noise(60, 10, 0, 0.5, 0.5, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_datn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
